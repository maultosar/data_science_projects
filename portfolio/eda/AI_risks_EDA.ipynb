{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitaliy-sharandin/data_science_projects/blob/master/portfolio/eda/AI_risks_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets\n",
        "\n",
        "* AI Incident Database <br>\n",
        "https://www.kaggle.com/datasets/konradb/ai-incident-database\n",
        "* Government AI Readiness Index <br>\n",
        "https://www.statista.com/statistics/1231685/worldwide-government-artificial-intelligence-readiness-index/ <br>\n",
        "https://www.statista.com/statistics/1231719/eastern-europe-government-artificial-intelligence-readiness-index/\n",
        "* AI Ethics Guidelines Global Inventory <br>\n",
        "https://www.statista.com/statistics/1286900/ai-ethics-principles-by-organization-type/\n",
        "\n",
        "\n",
        "# Questions\n",
        "\n",
        "1. What are main AI failure types?\n",
        "2. Failure types evolution through time.\n",
        "3. Failure types by domain/company/country/demographics/models.\n",
        "4. Consequences of AI failures\n",
        "5. How these AI risks map onto existing ethical guidelines and regulations?\n",
        "6. What are the mitigation strategies that are most effective?\n",
        "7. How to prevent those risks?\n",
        "\n"
      ],
      "metadata": {
        "id": "uuNNnS-p9i_K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDwiQ-Z059X7"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q datasets\n",
        "!pip install -U -q ydata-profiling\n",
        "!pip install -U -q keybert\n",
        "!pip install -U -q keyphrase-vectorizers\n",
        "!pip install -U -q spacy\n",
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from ydata_profiling import ProfileReport\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from keybert import KeyBERT\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy"
      ],
      "metadata": {
        "id": "WXDu6XcPB4VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "incident_dataset = load_dataset(\"vitaliy-sharandin/ai-incidents\")\n",
        "incident_dataset = incident_dataset['train'].to_pandas()"
      ],
      "metadata": {
        "id": "pL4B0QafB2y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(incident_dataset, title=\"Fraud data report\", dark_mode=True)\n",
        "profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "HOFU8dxNCR_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distribution of incidents over time"
      ],
      "metadata": {
        "id": "-McyFXUNbgtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Amount through time"
      ],
      "metadata": {
        "id": "qO4mtrYafzVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "incident_dataset['date'] = pd.to_datetime(incident_dataset['date'])\n",
        "incident_dataset['year'] = incident_dataset['date'].dt.year\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(incident_dataset['year'], bins=40, kde=True)\n",
        "plt.title('Distribution of AI Incidents Over Time')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4d87sGhiX7UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Insights\n",
        "* Early Years: There are very few incidents reported before the year 2010, indicating either a lack of reporting mechanisms or fewer AI deployments during that period.\n",
        "* Rapid Increase: There is a noticeable increase in the number of incidents starting from around 2015. This could correlate with the broader adoption of AI technologies in various industries.\n",
        "* Recent Years: The number of incidents seems to peak around 2020 and then shows a slight decline. This could be due to improved AI safety measures, changes in reporting, or other factors that need further investigation."
      ],
      "metadata": {
        "id": "j43h-JWwdFFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployers, developers"
      ],
      "metadata": {
        "id": "lKYSkbLRbqDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_occurrences(column):\n",
        "    items = column.str.replace('[\\[\\]\"]', '', regex=True).str.split(',')\n",
        "    counter = Counter([item.strip() for sublist in items.dropna() for item in sublist])\n",
        "    return counter\n",
        "\n",
        "deployer_counts = count_occurrences(incident_dataset['Alleged deployer of AI system'])\n",
        "developer_counts = count_occurrences(incident_dataset['Alleged developer of AI system'])\n",
        "\n",
        "\n",
        "deployer_df = pd.DataFrame(deployer_counts.items(), columns=['Deployer', 'Count']).sort_values('Count', ascending=False).head(30)\n",
        "developer_df = pd.DataFrame(developer_counts.items(), columns=['Developer', 'Count']).sort_values('Count', ascending=False).head(30)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 7))\n",
        "\n",
        "sns.barplot(x='Count', y='Deployer', data=deployer_df, ax=axes[0])\n",
        "axes[0].set_title('Top 10 Deployers Involved in AI Incidents')\n",
        "axes[0].set_xlabel('Number of Incidents')\n",
        "\n",
        "sns.barplot(x='Count', y='Developer', data=developer_df, ax=axes[1])\n",
        "axes[1].set_title('Top 10 Developers Involved in AI Incidents')\n",
        "axes[1].set_xlabel('Number of Incidents')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O30rCX-ycEQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Use Kmeans clustering to cluster into groups"
      ],
      "metadata": {
        "id": "Ejh1En54LMcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Insights\n",
        "\n",
        "  * Most Frequent Deployers: The company \"Facebook\" appears to be the most frequent deployer involved in AI incidents, followed by companies like \"Google\" and \"Microsoft\". This could indicate that platforms with large user bases and extensive AI deployments are more prone to incidents.\n",
        "  * Most Frequent Developers: Similarly, \"Facebook\" and \"Google\" are among the top developers involved in AI incidents. This isn't surprising given their role as major technology companies with extensive AI research and deployment.\n",
        "  * Tech Giants: Noticeably, many of the top deployers and developers are tech giants, which could imply a higher level of responsibility for these organizations in ensuring AI safety and ethics."
      ],
      "metadata": {
        "id": "3fSRlUa_dUza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Harmed parties analysis"
      ],
      "metadata": {
        "id": "4JIS-I_qGmSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "harmed_counts = count_occurrences(incident_dataset['Alleged harmed or nearly harmed parties'])\n",
        "\n",
        "harmed_df = pd.DataFrame(harmed_counts.items(), columns=['Harmed Party', 'Count']).sort_values('Count', ascending=False).head(20)\n",
        "\n",
        "plt.figure(figsize=(20, 7))\n",
        "sns.barplot(x='Count', y='Harmed Party', data=harmed_df)\n",
        "plt.title('Top 10 Most Commonly Harmed Parties in AI Incidents')\n",
        "plt.xlabel('Number of Incidents')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C81I3-RyGmGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Use Kmeans clustering to cluster into groups"
      ],
      "metadata": {
        "id": "OW4DaryeI5nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Insights\n",
        "\n",
        "\n",
        "  * General Users: The category \"facebook-users\" tops the list, indicating that general users of platforms like Facebook are most frequently impacted by AI incidents. This could reflect the broad user base and extensive AI systems deployed by such platforms.\n",
        "\n",
        "  * Children and Women: Both \"children\" and \"women\" appear in the list, suggesting that certain demographic groups may be disproportionately affected by AI incidents.\n",
        "\n",
        "  * Multiple Categories: We also see categories like \"pedestrians\" and \"drivers,\" indicating that AI systems deployed in transportation and public spaces have also led to incidents affecting these groups.\n",
        "\n",
        "  * Public and Private Entities: Various entities like \"employees,\" \"law-enforcement,\" and \"companies\" also make it to the list, indicating that the impact of AI incidents isn't limited to individual users but also extends to organizations and institutions."
      ],
      "metadata": {
        "id": "SUZNeOJCGteo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI failure types"
      ],
      "metadata": {
        "id": "9hJywVaf_fGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kw_model = KeyBERT()\n",
        "kph_vectorizer = KeyphraseCountVectorizer()\n",
        "\n",
        "kw_model = KeyBERT()\n",
        "st_model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "\n",
        "def pure_keybert(text, filter_phrase=\"inappropriate and failed\"):\n",
        "    keywords_tuples = kw_model.extract_keywords(text, vectorizer=kph_vectorizer)\n",
        "    keywords = [kw[0] for kw in keywords_tuples]\n",
        "\n",
        "    # Filter out single-word keywords, retain only multi-word keywords\n",
        "    multi_word_keywords = [kw for kw in keywords if ' ' in kw]\n",
        "\n",
        "    # If there are multi-word keywords, use them; otherwise, revert to single-word keywords\n",
        "    keywords_to_use = multi_word_keywords if multi_word_keywords else keywords\n",
        "\n",
        "    # keyword_embeddings = st_model.encode(keywords_to_use, convert_to_tensor=True)\n",
        "    # incident_reason_embedding = st_model.encode(filter_phrase, convert_to_tensor=True)\n",
        "\n",
        "    # similarities = [util.pytorch_cos_sim(keyword_embedding, incident_reason_embedding).item() for keyword_embedding in keyword_embeddings]\n",
        "\n",
        "    # max_similarity_index = similarities.index(max(similarities))\n",
        "    # most_relevant_keyword = keywords[max_similarity_index]\n",
        "\n",
        "    return keywords_to_use[0]\n",
        "\n",
        "def keybert_with_filter(text, filter_phrase=\"negative cause\"):\n",
        "    keywords_tuples = kw_model.extract_keywords(text, vectorizer=kph_vectorizer)\n",
        "    keywords = [kw[0] for kw in keywords_tuples]\n",
        "\n",
        "    keyword_embeddings = st_model.encode(keywords, convert_to_tensor=True)\n",
        "    incident_reason_embedding = st_model.encode(filter_phrase, convert_to_tensor=True)\n",
        "\n",
        "    similarities = [util.pytorch_cos_sim(keyword_embedding, incident_reason_embedding).item() for keyword_embedding in keyword_embeddings]\n",
        "\n",
        "    max_similarity_index = similarities.index(max(similarities))\n",
        "    most_relevant_keyword = keywords[max_similarity_index]\n",
        "\n",
        "    return most_relevant_keyword if similarities[max_similarity_index] > 0.5 else keywords[0]\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "def spacy_keyword(text, incident_verbs_list=[\"fail\", \"crash\", \"stop\", \"halt\", \"break\", \"malfunction\", \"kill\", \"die\", \"hurt\"], similarity_threshold=0.5):\n",
        "    doc = nlp(text)\n",
        "    causes = []\n",
        "    similar_verbs = []\n",
        "\n",
        "    # Identify similar or same words to those in the incident_verbs_list\n",
        "    for token in doc:\n",
        "        for incident_verb in incident_verbs_list:\n",
        "            if token.similarity(nlp(incident_verb)) > similarity_threshold:\n",
        "                similar_verbs.append(token)\n",
        "                break\n",
        "\n",
        "    # Extract related noun chunks for identified similar verbs\n",
        "    for token in similar_verbs:\n",
        "        for chunk in doc.noun_chunks:\n",
        "            if token in list(chunk.root.children) or token == chunk.root:\n",
        "                causes.append(chunk.text)\n",
        "\n",
        "    return causes\n",
        "\n",
        "incident_dataset['pure_keybert'] = incident_dataset['description'].head(30).apply(pure_keybert)\n",
        "incident_dataset['keybert_with_filter'] = incident_dataset['description'].head(30).apply(keybert_with_filter)\n",
        "incident_dataset['spacy'] = incident_dataset['description'].head(30).apply(spacy_keyword)\n",
        "incident_dataset[['description', 'pure_keybert','keybert_with_filter', 'spacy']]"
      ],
      "metadata": {
        "id": "9dY8F9PN_vQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_impacted_entities(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Using NER to get potential entities\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"PERSON\"]]\n",
        "\n",
        "    # Using dependency parsing to refine the entities list\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "\n",
        "    # Define a pattern to match verbs like 'affect', 'impact', etc. and their objects\n",
        "    pattern = [{\"LEMMA\": {\"IN\": [\"affect\", \"impact\", \"hurt\"]}},\n",
        "               {\"POS\": \"ADP\", \"OP\": \"?\"},  # optional preposition\n",
        "               {\"POS\": \"NOUN\", \"OP\": \"?\"}, # optional noun\n",
        "               {\"POS\": \"DET\", \"OP\": \"?\"},  # optional determiner\n",
        "               {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}, \"OP\": \"+\"}]  # nouns or proper nouns\n",
        "\n",
        "    matcher.add(\"IMPACTED_ENTITY_PATTERN\", [pattern])\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    # Extracting matched entities\n",
        "    for match_id, start, end in matches:\n",
        "        span = doc[start:end]\n",
        "        entities.append(span.text)\n",
        "\n",
        "    # Return unique entities\n",
        "    return list(set(entities))\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2Ex2xWvB6Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Word cloud from keybert/ keyphrases. Is WordCloud used for phrases?\n",
        "2. Clustering of results into groups\n",
        "\n"
      ],
      "metadata": {
        "id": "QYU-Om46MOD6"
      }
    }
  ]
}