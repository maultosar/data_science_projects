{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitaliy-sharandin/data_science_projects/blob/master/portfolio/eda/AI_risks_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI risks EDA\n",
        "This project provides analysis of AI related incidents throughout the years."
      ],
      "metadata": {
        "id": "rbhkAXpA_VEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets\n",
        "The dataset used in project is based on the website aggregating AI incidents:\n",
        "https://incidentdatabase.ai/\n",
        "\n",
        "\n",
        "* AI Incident Database <br>\n",
        "https://www.kaggle.com/datasets/konradb/ai-incident-database\n"
      ],
      "metadata": {
        "id": "uuNNnS-p9i_K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MDwiQ-Z059X7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ed4c65-3a1c-4d03-f591-677c2ead2c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-13 10:20:19.990629: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-13 10:20:22.321055: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.0/en_core_web_lg-3.7.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.0) (3.7.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.3.2)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q datasets\n",
        "!pip install -U -q ydata-profiling\n",
        "!pip install -U -q keybert\n",
        "!pip install -U -q keyphrase-vectorizers\n",
        "!pip install -U -q spacy\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import typing_extensions\n",
        "from ydata_profiling import ProfileReport\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from keybert import KeyBERT\n",
        "from keyphrase_vectorizers import KeyphraseTfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import typing_extensions"
      ],
      "metadata": {
        "id": "WXDu6XcPB4VU",
        "outputId": "206e044a-0b4d-4a52-e5fc-8ca52572f007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-43af91678926>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping_extensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mydata_profiling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProfileReport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ydata_profiling/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m  \u001b[0;31m# isort:skip # noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mydata_profiling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompare_reports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompare\u001b[0m  \u001b[0;31m# isort:skip # noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mydata_profiling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontroller\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas_decorator\u001b[0m  \u001b[0;31m# isort:skip # noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mydata_profiling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile_report\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProfileReport\u001b[0m  \u001b[0;31m# isort:skip # noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ydata_profiling/compare_reports.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mydata_profiling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseDescription\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mydata_profiling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malerts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mydata_profiling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile_report\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProfileReport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ydata_profiling/profile_report.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtypeguard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypechecked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvisions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisionsTypeset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/typeguard/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_type\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcheck_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarn_on_error\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwarn_on_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_importhook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImportHookManager\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mImportHookManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_importhook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTypeguardFinder\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mTypeguardFinder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_importhook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minstall_import_hook\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minstall_import_hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/typeguard/_importhook.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Buffer' from 'typing_extensions' (/usr/local/lib/python3.10/dist-packages/typing_extensions.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip freeze typing_extensions"
      ],
      "metadata": {
        "id": "77RD5C4AgiaX",
        "outputId": "bccbcf84-b40e-4dca-ee38-6b0f0995e489",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absl-py==1.4.0\n",
            "aiohttp==3.8.6\n",
            "aiosignal==1.3.1\n",
            "alabaster==0.7.13\n",
            "albumentations==1.3.1\n",
            "altair==4.2.2\n",
            "anyio==3.7.1\n",
            "appdirs==1.4.4\n",
            "argon2-cffi==23.1.0\n",
            "argon2-cffi-bindings==21.2.0\n",
            "array-record==0.4.1\n",
            "arviz==0.15.1\n",
            "astropy==5.3.4\n",
            "astunparse==1.6.3\n",
            "async-timeout==4.0.3\n",
            "attrs==23.1.0\n",
            "audioread==3.0.1\n",
            "autograd==1.6.2\n",
            "Babel==2.13.0\n",
            "backcall==0.2.0\n",
            "beautifulsoup4==4.11.2\n",
            "bleach==6.1.0\n",
            "blinker==1.4\n",
            "blis==0.7.11\n",
            "blosc2==2.0.0\n",
            "bokeh==3.2.2\n",
            "bqplot==0.12.40\n",
            "branca==0.6.0\n",
            "build==1.0.3\n",
            "CacheControl==0.13.1\n",
            "cachetools==5.3.1\n",
            "catalogue==2.0.10\n",
            "certifi==2023.7.22\n",
            "cffi==1.16.0\n",
            "chardet==5.2.0\n",
            "charset-normalizer==3.3.0\n",
            "chex==0.1.7\n",
            "click==8.1.7\n",
            "click-plugins==1.1.1\n",
            "cligj==0.7.2\n",
            "cloudpathlib==0.15.1\n",
            "cloudpickle==2.2.1\n",
            "cmake==3.27.6\n",
            "cmdstanpy==1.2.0\n",
            "colorcet==3.0.1\n",
            "colorlover==0.3.0\n",
            "colour==0.1.5\n",
            "community==1.0.0b1\n",
            "confection==0.1.3\n",
            "cons==0.4.6\n",
            "contextlib2==21.6.0\n",
            "contourpy==1.1.1\n",
            "convertdate==2.4.0\n",
            "cryptography==41.0.4\n",
            "cufflinks==0.17.3\n",
            "cupy-cuda11x==11.0.0\n",
            "cvxopt==1.3.2\n",
            "cvxpy==1.3.2\n",
            "cycler==0.12.1\n",
            "cymem==2.0.8\n",
            "Cython==3.0.3\n",
            "dacite==1.8.1\n",
            "dask==2023.8.1\n",
            "datascience==0.17.6\n",
            "datasets==2.14.5\n",
            "db-dtypes==1.1.1\n",
            "dbus-python==1.2.18\n",
            "debugpy==1.6.6\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "dill==0.3.7\n",
            "distributed==2023.8.1\n",
            "distro==1.7.0\n",
            "dlib==19.24.2\n",
            "dm-tree==0.1.8\n",
            "docutils==0.18.1\n",
            "dopamine-rl==4.0.6\n",
            "duckdb==0.8.1\n",
            "earthengine-api==0.1.374\n",
            "easydict==1.10\n",
            "ecos==2.0.12\n",
            "editdistance==0.6.2\n",
            "eerepr==0.0.4\n",
            "en-core-web-lg @ https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.0/en_core_web_lg-3.7.0-py3-none-any.whl#sha256=708da1110fbe1163d059de34a2cbedb1db65c26e1e624ca925897a2711cb7d77\n",
            "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl#sha256=83276fc78a70045627144786b52e1f2728ad5e29e5e43916ec37ea9c26a11212\n",
            "entrypoints==0.4\n",
            "ephem==4.1.5\n",
            "et-xmlfile==1.1.0\n",
            "etils==1.5.0\n",
            "etuples==0.3.9\n",
            "exceptiongroup==1.1.3\n",
            "fastai==2.7.12\n",
            "fastcore==1.5.29\n",
            "fastdownload==0.0.7\n",
            "fastjsonschema==2.18.1\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8.2\n",
            "filelock==3.12.4\n",
            "Fiona==1.9.4.post1\n",
            "firebase-admin==5.3.0\n",
            "Flask==2.2.5\n",
            "flatbuffers==23.5.26\n",
            "flax==0.7.4\n",
            "folium==0.14.0\n",
            "fonttools==4.43.1\n",
            "frozendict==2.3.8\n",
            "frozenlist==1.4.0\n",
            "fsspec==2023.6.0\n",
            "future==0.18.3\n",
            "gast==0.4.0\n",
            "gcsfs==2023.6.0\n",
            "GDAL==3.4.3\n",
            "gdown==4.6.6\n",
            "geemap==0.28.2\n",
            "gensim==4.3.2\n",
            "geocoder==1.38.1\n",
            "geographiclib==2.0\n",
            "geopandas==0.13.2\n",
            "geopy==2.3.0\n",
            "gin-config==0.5.0\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-api-core==2.11.1\n",
            "google-api-python-client==2.84.0\n",
            "google-auth==2.17.3\n",
            "google-auth-httplib2==0.1.1\n",
            "google-auth-oauthlib==1.0.0\n",
            "google-cloud-bigquery==3.10.0\n",
            "google-cloud-bigquery-connection==1.12.1\n",
            "google-cloud-bigquery-storage==2.22.0\n",
            "google-cloud-core==2.3.3\n",
            "google-cloud-datastore==2.15.2\n",
            "google-cloud-firestore==2.11.1\n",
            "google-cloud-functions==1.13.3\n",
            "google-cloud-language==2.9.1\n",
            "google-cloud-storage==2.8.0\n",
            "google-cloud-translate==3.11.3\n",
            "google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz#sha256=a9ae86f42f08e9abe9f8f335e85f6288917a4ef7d3e941260cddba68c57ca648\n",
            "google-crc32c==1.5.0\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.6.0\n",
            "googleapis-common-protos==1.60.0\n",
            "googledrivedownloader==0.4\n",
            "graphviz==0.20.1\n",
            "greenlet==3.0.0\n",
            "grpc-google-iam-v1==0.12.6\n",
            "grpcio==1.59.0\n",
            "grpcio-status==1.48.2\n",
            "gspread==3.4.2\n",
            "gspread-dataframe==3.3.1\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "h5netcdf==1.2.0\n",
            "h5py==3.9.0\n",
            "holidays==0.34\n",
            "holoviews==1.17.1\n",
            "html5lib==1.1\n",
            "htmlmin==0.1.12\n",
            "httpimport==1.3.1\n",
            "httplib2==0.22.0\n",
            "huggingface-hub==0.17.3\n",
            "humanize==4.7.0\n",
            "hyperopt==0.2.7\n",
            "idna==3.4\n",
            "ImageHash==4.3.1\n",
            "imageio==2.31.5\n",
            "imageio-ffmpeg==0.4.9\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.10.1\n",
            "imgaug==0.4.0\n",
            "importlib-metadata==6.8.0\n",
            "importlib-resources==6.1.0\n",
            "imutils==0.5.4\n",
            "inflect==7.0.0\n",
            "iniconfig==2.0.0\n",
            "intel-openmp==2023.2.0\n",
            "ipyevents==2.0.2\n",
            "ipyfilechooser==0.6.0\n",
            "ipykernel==5.5.6\n",
            "ipyleaflet==0.17.4\n",
            "ipython==7.34.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.5.0\n",
            "ipytree==0.2.2\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==2.1.2\n",
            "jax==0.4.16\n",
            "jaxlib @ https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.16+cuda11.cudnn86-cp310-cp310-manylinux2014_x86_64.whl#sha256=78b3a9acfda4bfaae8a1dc112995d56454020f5c02dba4d24c40c906332efd4a\n",
            "jeepney==0.7.1\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.2\n",
            "joblib==1.3.2\n",
            "jsonpickle==3.0.2\n",
            "jsonschema==4.19.1\n",
            "jsonschema-specifications==2023.7.1\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter-server==1.24.0\n",
            "jupyter_core==5.3.2\n",
            "jupyterlab-pygments==0.2.2\n",
            "jupyterlab-widgets==3.0.9\n",
            "kaggle==1.5.16\n",
            "keras==2.13.1\n",
            "keybert==0.8.3\n",
            "keyphrase-vectorizers==0.0.11\n",
            "keyring==23.5.0\n",
            "kiwisolver==1.4.5\n",
            "langcodes==3.3.0\n",
            "launchpadlib==1.10.16\n",
            "lazr.restfulclient==0.14.4\n",
            "lazr.uri==1.0.6\n",
            "lazy_loader==0.3\n",
            "libclang==16.0.6\n",
            "librosa==0.10.1\n",
            "lightgbm==4.0.0\n",
            "linkify-it-py==2.0.2\n",
            "lit==17.0.2\n",
            "llvmlite==0.39.1\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.6\n",
            "LunarCalendar==0.0.9\n",
            "lxml==4.9.3\n",
            "malloy==2023.1056\n",
            "Markdown==3.5\n",
            "markdown-it-py==3.0.0\n",
            "MarkupSafe==2.1.3\n",
            "matplotlib==3.7.1\n",
            "matplotlib-inline==0.1.6\n",
            "matplotlib-venn==0.11.9\n",
            "mdit-py-plugins==0.4.0\n",
            "mdurl==0.1.2\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.2\n",
            "mistune==0.8.4\n",
            "mizani==0.9.3\n",
            "mkl==2023.2.0\n",
            "ml-dtypes==0.3.1\n",
            "mlxtend==0.22.0\n",
            "more-itertools==10.1.0\n",
            "moviepy==1.0.3\n",
            "mpmath==1.3.0\n",
            "msgpack==1.0.7\n",
            "multidict==6.0.4\n",
            "multimethod==1.10\n",
            "multipledispatch==1.0.0\n",
            "multiprocess==0.70.15\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.10\n",
            "music21==9.1.0\n",
            "natsort==8.4.0\n",
            "nbclassic==1.0.0\n",
            "nbclient==0.8.0\n",
            "nbconvert==6.5.4\n",
            "nbformat==5.9.2\n",
            "nest-asyncio==1.5.8\n",
            "networkx==3.1\n",
            "nibabel==4.0.2\n",
            "nltk==3.8.1\n",
            "notebook==6.5.5\n",
            "notebook_shim==0.2.3\n",
            "numba==0.56.4\n",
            "numexpr==2.8.7\n",
            "numpy==1.23.5\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.2.2\n",
            "opencv-contrib-python==4.8.0.76\n",
            "opencv-python==4.8.0.76\n",
            "opencv-python-headless==4.8.1.78\n",
            "openpyxl==3.1.2\n",
            "opt-einsum==3.3.0\n",
            "optax==0.1.7\n",
            "orbax-checkpoint==0.4.1\n",
            "osqp==0.6.2.post8\n",
            "packaging==23.2\n",
            "pandas==1.5.3\n",
            "pandas-datareader==0.10.0\n",
            "pandas-gbq==0.17.9\n",
            "pandocfilters==1.5.0\n",
            "panel==1.2.3\n",
            "param==1.13.0\n",
            "parso==0.8.3\n",
            "partd==1.4.1\n",
            "pathlib==1.0.1\n",
            "pathy==0.10.2\n",
            "patsy==0.5.3\n",
            "peewee==3.16.3\n",
            "pexpect==4.8.0\n",
            "phik==0.12.3\n",
            "pickleshare==0.7.5\n",
            "Pillow==9.4.0\n",
            "pip-tools==6.13.0\n",
            "platformdirs==3.11.0\n",
            "plotly==5.15.0\n",
            "plotnine==0.12.3\n",
            "pluggy==1.3.0\n",
            "polars==0.17.3\n",
            "pooch==1.7.0\n",
            "portpicker==1.5.2\n",
            "prefetch-generator==1.0.3\n",
            "preshed==3.0.9\n",
            "prettytable==3.9.0\n",
            "proglog==0.1.10\n",
            "progressbar2==4.2.0\n",
            "prometheus-client==0.17.1\n",
            "promise==2.3\n",
            "prompt-toolkit==3.0.39\n",
            "prophet==1.1.4\n",
            "proto-plus==1.22.3\n",
            "protobuf==3.20.3\n",
            "psutil==5.9.5\n",
            "psycopg2==2.9.9\n",
            "ptyprocess==0.7.0\n",
            "py-cpuinfo==9.0.0\n",
            "py4j==0.10.9.7\n",
            "pyarrow==9.0.0\n",
            "pyasn1==0.5.0\n",
            "pyasn1-modules==0.3.0\n",
            "pycocotools==2.0.7\n",
            "pycparser==2.21\n",
            "pyct==0.5.0\n",
            "pydantic==1.10.13\n",
            "pydata-google-auth==1.8.2\n",
            "pydot==1.4.2\n",
            "pydot-ng==2.0.0\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "PyDrive2==1.6.3\n",
            "pyerfa==2.0.0.3\n",
            "pygame==2.5.2\n",
            "Pygments==2.16.1\n",
            "PyGObject==3.42.1\n",
            "PyJWT==2.3.0\n",
            "pymc==5.7.2\n",
            "PyMeeus==0.5.12\n",
            "pymystem3==0.2.0\n",
            "PyOpenGL==3.1.7\n",
            "pyOpenSSL==23.2.0\n",
            "pyparsing==3.1.1\n",
            "pyperclip==1.8.2\n",
            "pyproj==3.6.1\n",
            "pyproject_hooks==1.0.0\n",
            "pyshp==2.3.1\n",
            "PySocks==1.7.1\n",
            "pytensor==2.14.2\n",
            "pytest==7.4.2\n",
            "python-apt==0.0.0\n",
            "python-box==7.1.1\n",
            "python-dateutil==2.8.2\n",
            "python-louvain==0.16\n",
            "python-slugify==8.0.1\n",
            "python-utils==3.8.1\n",
            "pytz==2023.3.post1\n",
            "pyviz_comms==3.0.0\n",
            "PyWavelets==1.4.1\n",
            "PyYAML==6.0.1\n",
            "pyzmq==23.2.1\n",
            "qdldl==0.1.7.post0\n",
            "qudida==0.0.4\n",
            "ratelim==0.1.6\n",
            "referencing==0.30.2\n",
            "regex==2023.6.3\n",
            "requests==2.31.0\n",
            "requests-oauthlib==1.3.1\n",
            "requirements-parser==0.5.0\n",
            "rich==13.6.0\n",
            "rpds-py==0.10.4\n",
            "rpy2==3.4.2\n",
            "rsa==4.9\n",
            "safetensors==0.4.0\n",
            "scikit-image==0.19.3\n",
            "scikit-learn==1.2.2\n",
            "scipy==1.11.3\n",
            "scooby==0.7.4\n",
            "scs==3.2.3\n",
            "seaborn==0.12.2\n",
            "SecretStorage==3.3.1\n",
            "Send2Trash==1.8.2\n",
            "sentence-transformers==2.2.2\n",
            "sentencepiece==0.1.99\n",
            "shapely==2.0.1\n",
            "six==1.16.0\n",
            "sklearn-pandas==2.2.0\n",
            "smart-open==6.4.0\n",
            "sniffio==1.3.0\n",
            "snowballstemmer==2.2.0\n",
            "sortedcontainers==2.4.0\n",
            "soundfile==0.12.1\n",
            "soupsieve==2.5\n",
            "soxr==0.3.7\n",
            "spacy==3.7.1\n",
            "spacy-alignments==0.9.1\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.5\n",
            "spacy-transformers==1.3.2\n",
            "Sphinx==5.0.2\n",
            "sphinxcontrib-applehelp==1.0.7\n",
            "sphinxcontrib-devhelp==1.0.5\n",
            "sphinxcontrib-htmlhelp==2.0.4\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==1.0.6\n",
            "sphinxcontrib-serializinghtml==1.1.9\n",
            "SQLAlchemy==2.0.21\n",
            "sqlparse==0.4.4\n",
            "srsly==2.4.8\n",
            "stanio==0.3.0\n",
            "statsmodels==0.14.0\n",
            "sympy==1.12\n",
            "tables==3.8.0\n",
            "tabulate==0.9.0\n",
            "tangled-up-in-unicode==0.2.0\n",
            "tbb==2021.10.0\n",
            "tblib==2.0.0\n",
            "tenacity==8.2.3\n",
            "tensorboard==2.13.0\n",
            "tensorboard-data-server==0.7.1\n",
            "tensorflow==2.13.0\n",
            "tensorflow-datasets==4.9.3\n",
            "tensorflow-estimator==2.13.0\n",
            "tensorflow-gcs-config==2.13.0\n",
            "tensorflow-hub==0.15.0\n",
            "tensorflow-io-gcs-filesystem==0.34.0\n",
            "tensorflow-metadata==1.14.0\n",
            "tensorflow-probability==0.20.1\n",
            "tensorstore==0.1.45\n",
            "termcolor==2.3.0\n",
            "terminado==0.17.1\n",
            "text-unidecode==1.3\n",
            "textblob==0.17.1\n",
            "tf-slim==1.1.0\n",
            "thinc==8.1.12\n",
            "threadpoolctl==3.2.0\n",
            "tifffile==2023.9.26\n",
            "tinycss2==1.2.1\n",
            "tokenizers==0.14.1\n",
            "toml==0.10.2\n",
            "tomli==2.0.1\n",
            "toolz==0.12.0\n",
            "torch @ https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl#sha256=a7a49d459bf4862f64f7bc1a68beccf8881c2fa9f3e0569608e16ba6f85ebf7b\n",
            "torchaudio @ https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp310-cp310-linux_x86_64.whl#sha256=26692645ea061a005c57ec581a2d0425210ac6ba9f923edf11cc9b0ef3a111e9\n",
            "torchdata==0.6.1\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.15.2\n",
            "torchvision @ https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp310-cp310-linux_x86_64.whl#sha256=19ca4ab5d6179bbe53cff79df1a855ee6533c2861ddc7389f68349d8b9f8302a\n",
            "tornado==6.3.2\n",
            "tqdm==4.66.1\n",
            "traitlets==5.7.1\n",
            "traittypes==0.2.1\n",
            "transformers==4.34.0\n",
            "triton==2.0.0\n",
            "tweepy==4.13.0\n",
            "typeguard==4.1.5\n",
            "typer==0.9.0\n",
            "types-setuptools==68.2.0.0\n",
            "typing_extensions==4.8.0\n",
            "tzlocal==5.1\n",
            "uc-micro-py==1.0.2\n",
            "uritemplate==4.1.1\n",
            "urllib3==2.0.6\n",
            "vega-datasets==0.9.0\n",
            "visions==0.7.5\n",
            "wadllib==1.3.6\n",
            "wasabi==1.1.2\n",
            "wcwidth==0.2.8\n",
            "weasel==0.3.2\n",
            "webcolors==1.13\n",
            "webencodings==0.5.1\n",
            "websocket-client==1.6.4\n",
            "Werkzeug==3.0.0\n",
            "widgetsnbextension==3.6.6\n",
            "wordcloud==1.9.2\n",
            "wrapt==1.15.0\n",
            "xarray==2023.7.0\n",
            "xarray-einstats==0.6.0\n",
            "xgboost==2.0.0\n",
            "xlrd==2.0.1\n",
            "xxhash==3.4.1\n",
            "xyzservices==2023.10.0\n",
            "yarl==1.9.2\n",
            "ydata-profiling==4.6.0\n",
            "yellowbrick==1.5\n",
            "yfinance==0.2.31\n",
            "zict==3.0.0\n",
            "zipp==3.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "incident_dataset = load_dataset(\"vitaliy-sharandin/ai-incidents\")\n",
        "incident_dataset = incident_dataset['train'].to_pandas()"
      ],
      "metadata": {
        "id": "pL4B0QafB2y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(incident_dataset, title=\"Fraud data report\", dark_mode=True)\n",
        "profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "HOFU8dxNCR_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distribution of incidents over time"
      ],
      "metadata": {
        "id": "-McyFXUNbgtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Amount of incidents through time"
      ],
      "metadata": {
        "id": "qO4mtrYafzVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "incident_dataset['date'] = pd.to_datetime(incident_dataset['date'])\n",
        "incident_dataset['year'] = incident_dataset['date'].dt.year\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(incident_dataset['year'], bins=40, kde=True)\n",
        "plt.title('Distribution of AI Incidents Over Time')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4d87sGhiX7UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights**\n",
        "* Early Years <br>\n",
        "There are very few incidents reported before the year 2010 which can be related to the fact that deep learning models boom happened in early 2010s.\n",
        "\n",
        "* Rapid Increase <br>\n",
        "There is a noticeable increase in the number of incidents starting from around 2015. This correlates with wider adoption of models as well as development of advanced programs such as AlphaGo and introduction of transformer arhitecture in 2017."
      ],
      "metadata": {
        "id": "j43h-JWwdFFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "earliest_incident_year = incident_dataset['year'].min()\n",
        "incident_dataset[incident_dataset['year']==earliest_incident_year]"
      ],
      "metadata": {
        "id": "aLdNR6jlH2bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Earliest incident happended in 1983 when Stanislav Petrov identified incoming intercontinental ballistic missiles shown by the automatic aerial strike warning system flying at Soviet Union as false positive, preventing nuclear armageddon."
      ],
      "metadata": {
        "id": "DBPMh5-bHwjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployers and developers of models"
      ],
      "metadata": {
        "id": "lKYSkbLRbqDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_occurrences(column):\n",
        "    items = column.str.replace('[\\[\\]\"]', '', regex=True).str.split(',')\n",
        "    counter = Counter([item.strip() for sublist in items.dropna() for item in sublist])\n",
        "    return counter\n",
        "\n",
        "deployer_counts = count_occurrences(incident_dataset['Alleged deployer of AI system'])\n",
        "developer_counts = count_occurrences(incident_dataset['Alleged developer of AI system'])\n",
        "\n",
        "\n",
        "deployer_df = pd.DataFrame(deployer_counts.items(), columns=['Deployer', 'Count']).sort_values('Count', ascending=False).head(30)\n",
        "developer_df = pd.DataFrame(developer_counts.items(), columns=['Developer', 'Count']).sort_values('Count', ascending=False).head(30)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 7))\n",
        "\n",
        "sns.barplot(x='Count', y='Deployer', data=deployer_df, ax=axes[0])\n",
        "axes[0].set_title('Top 10 Deployers Involved in AI Incidents')\n",
        "axes[0].set_xlabel('Number of Incidents')\n",
        "\n",
        "sns.barplot(x='Count', y='Developer', data=developer_df, ax=axes[1])\n",
        "axes[1].set_title('Top 10 Developers Involved in AI Incidents')\n",
        "axes[1].set_xlabel('Number of Incidents')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O30rCX-ycEQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights**\n",
        "\n",
        "  * \"Facebook\" appears to be both the most frequent deployer involved in AI incidents, followed by \"Tesla\" and \"Google\", as platforms with large user bases and extensive AI deployments are more prone to incidents, especially Facebook which knowingly feeds their machine learning models with user data.\n",
        "\n",
        "  * \"Facebook\", \"Tesla\", \"OpenAI\" and \"Google\" are among the top developers involved in AI incidents. This isn't surprising given their role as major technology companies with extensive AI research and deployment.\n",
        "  \n",
        "  * Noticeably, many of the top deployers and developers are tech giants, which could imply a higher level of responsibility for these organizations in ensuring AI safety and ethics."
      ],
      "metadata": {
        "id": "3fSRlUa_dUza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Harmed parties"
      ],
      "metadata": {
        "id": "4JIS-I_qGmSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "harmed_counts = count_occurrences(incident_dataset['Alleged harmed or nearly harmed parties'])\n",
        "\n",
        "harmed_df = pd.DataFrame(harmed_counts.items(), columns=['Harmed Party', 'Count']).sort_values('Count', ascending=False).head(20)\n",
        "\n",
        "plt.figure(figsize=(20, 7))\n",
        "sns.barplot(x='Count', y='Harmed Party', data=harmed_df)\n",
        "plt.title('Top 10 Most Commonly Harmed Parties in AI Incidents')\n",
        "plt.xlabel('Number of Incidents')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C81I3-RyGmGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights**\n",
        "\n",
        "\n",
        "  * The category \"facebook-users\" tops the list which correlates with what we saw previously in deployers and developers analysis, meaning Facebook is harming its own users. Also, we can see that other big platform users are often harmed by their owners.\n",
        "\n",
        "  * We can see that minority groups in general, racial groups such as Jewish and Black as well as women are affected by biases of AI.\n",
        "\n",
        "  * Categories like \"pedestrians\", \"tesla-drivers\" and \"motorists\" are contributing to incidents in automotive industry."
      ],
      "metadata": {
        "id": "SUZNeOJCGteo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI incident types"
      ],
      "metadata": {
        "id": "9hJywVaf_fGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Incident keywords extraction"
      ],
      "metadata": {
        "id": "Jx_X-hARhwsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below you can see the keywords extraction from either title or description of incident indicating what was the most probable reason of incident in one phrase or word.<br>\n",
        "Here `KeyBert` together with `KeyphraseTfidfVectorizer` were used for finding max similarity to the words in the filtering list containning different generalized incident phrases. Manual verification of result was used for calibration of this method which in the end performs pretty well."
      ],
      "metadata": {
        "id": "aI_FCZNBV130"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import traceback\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "kw_model = KeyBERT()\n",
        "kph_vectorizer = KeyphraseTfidfVectorizer(spacy_pipeline=nlp, pos_pattern = '<J.*>*<V.*>*<N.*>*')\n",
        "\n",
        "st_model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "\n",
        "# keyword filter\n",
        "failures_filter = [\"wrong action\",\"negative cause\",\"failure reason\",\"AI failure\",\"software failure\",\n",
        "                  \"bad\", \"fail\", \"crash\", \"stop\", \"halt\", \"break\", \"malfunction\", \"kill\", \"die\",\n",
        "                  \"hurt\",\"bias\",\"disrespect\",\"discrimination\",\"racism\",\"sexism\",\"wrong\",\"error\"]\n",
        "\n",
        "def keybert_failures_combined(row, similarity_threshold=0.6, filter_phrases=failures_filter):\n",
        "\n",
        "  def get_most_relevant(text):\n",
        "      keywords_tuples = kw_model.extract_keywords(text, vectorizer=kph_vectorizer)\n",
        "      keywords = [kw[0] for kw in keywords_tuples if kw]\n",
        "\n",
        "      keyword_embeddings = st_model.encode(keywords, convert_to_tensor=True)\n",
        "      filter_phrase_embeddings = st_model.encode(filter_phrases, convert_to_tensor=True)\n",
        "\n",
        "      max_similarities = [max([util.pytorch_cos_sim(keyword_embedding, filter_phrase_embedding).item()\n",
        "                                for filter_phrase_embedding in filter_phrase_embeddings])\n",
        "                          for keyword_embedding in keyword_embeddings]\n",
        "\n",
        "      if not max_similarities:\n",
        "          return None, 0\n",
        "\n",
        "      most_relevant_keyword = keywords[max_similarities.index(max(max_similarities))]\n",
        "      return most_relevant_keyword, max(max_similarities)\n",
        "\n",
        "  desc_keyword, desc_similarity = get_most_relevant(row['description'])\n",
        "  title_keyword, title_similarity = get_most_relevant(row['title'])\n",
        "\n",
        "  return desc_keyword if desc_similarity >= title_similarity else title_keyword\n",
        "\n",
        "incident_dataset['ai_failures_summary'] = incident_dataset.apply(keybert_failures_combined, axis=1)\n",
        "\n",
        "incident_dataset[['description','title','ai_failures_summary']]"
      ],
      "metadata": {
        "id": "9dY8F9PN_vQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Incidents cloud"
      ],
      "metadata": {
        "id": "XB7tRsUBhmN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create a cloud of words to see the most frequent out of those incident reasons we have found in previous step as a quick and simple glimpse onto main types of incidents."
      ],
      "metadata": {
        "id": "-hI5jI1QaZWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "    background_color='black',\n",
        "    max_words=100,\n",
        "    width=800,\n",
        "    height=400\n",
        ").generate(' '.join(incident_dataset['ai_failures_summary']))\n",
        "\n",
        "# Plotting the word cloud\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud for Incident Keywords')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ygX_uTlKgTbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cluster analysis\n",
        "Now let's form groups out of incident reasons with cluster analysis."
      ],
      "metadata": {
        "id": "Eap_v8_ibOfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def get_single_term_for_cluster(data, clusters, labels):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of cluster centers (mean TF-IDF vector of each cluster) and their top keyword.\n",
        "    It aims to capture the most distinguishing feature of each cluster.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n",
        "    overall_mean = df.mean(axis=0)\n",
        "\n",
        "    top_keywords = {}\n",
        "\n",
        "    for i, r in df.iterrows():\n",
        "        relative_difference = r - overall_mean\n",
        "        top_keywords[i] = labels[np.argmax(relative_difference)]\n",
        "\n",
        "    return top_keywords\n",
        "\n",
        "def tokenize_and_filter(sentence):\n",
        "  tokens = nlp(sentence)\n",
        "  return [token.text for token in tokens if token.pos_ in [\"NOUN\", \"ADJ\", \"ADV\", \"VERB\"]]\n",
        "\n",
        "def cluster_and_visualize(df, column=\"description\", n_clusters=50):\n",
        "    # Vectorization\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_df=0.85, max_features=10000, tokenizer=tokenize_and_filter)\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(df[column])\n",
        "\n",
        "    # KMeans Clustering\n",
        "    km = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    clusters = km.fit_predict(tfidf_matrix)\n",
        "\n",
        "    display(clusters)\n",
        "\n",
        "    # Determine Cluster Names\n",
        "    cluster_keywords = get_single_term_for_cluster(tfidf_matrix, clusters, tfidf_vectorizer.get_feature_names_out())\n",
        "    df['Cluster'] = [cluster_keywords[cluster] for cluster in clusters]\n",
        "\n",
        "    display(df['Cluster'])\n",
        "\n",
        "    # Visualization - PCA plot\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_features = pca.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=clusters, cmap='viridis', s=50)\n",
        "    plt.title('PCA Visualization of AI Failure Descriptions Clusters')\n",
        "    plt.xlabel('PCA 1')\n",
        "    plt.ylabel('PCA 2')\n",
        "    legend_labels = [cluster_keywords[i] for i in range(n_clusters)]\n",
        "    plt.legend(handles=scatter.legend_elements()[0], labels=legend_labels, title=\"Clusters\")\n",
        "    plt.show()\n",
        "\n",
        "    # Visualization - Cluster Count plot\n",
        "    top_clusters = df['Cluster'].value_counts().head(10).reset_index()\n",
        "    top_clusters.columns = ['Cluster', 'Count']\n",
        "\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    sns_barplot = sns.barplot(x='Count', y='Cluster', data=top_clusters)\n",
        "    plt.title('Counts of Top 10 AI Failure Descriptions Clusters')\n",
        "    plt.xlabel(\"Count\")\n",
        "    plt.ylabel(\"Cluster\")\n",
        "\n",
        "    # Display counts on the bars\n",
        "    for index, value in enumerate(top_clusters['Count']):\n",
        "        sns_barplot.text(value, index, f' {value}', color='black', ha=\"left\", va=\"center\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return df\n",
        "\n",
        "incident_dataset = cluster_and_visualize(incident_dataset, column='ai_failures_summary')"
      ],
      "metadata": {
        "id": "UJZcm08WEP_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights**\n",
        "\n",
        "We can see that main AI failures are caused by:\n",
        "\n",
        "* Inaccuracy\n",
        "* Unrealiability\n",
        "* Bias\n",
        "* Falsehood\n",
        "* Crashes\n",
        "* Company related activity\n",
        "* Racism\n",
        "\n"
      ],
      "metadata": {
        "id": "SM60EqvnfIE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clusters through time\n",
        "\n"
      ],
      "metadata": {
        "id": "QYU-Om46MOD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_clusters_stacked_bar_yearly(df, date_column=\"date\"):\n",
        "    # Extract year from 'date' column\n",
        "    df['Year'] = pd.to_datetime(df[date_column]).dt.year\n",
        "\n",
        "    # Extract top 10 clusters\n",
        "    top_clusters = df['Cluster'].value_counts().head(10).index.tolist()\n",
        "\n",
        "    # Filter dataframe to only include top 10 clusters\n",
        "    top_clusters_df = df[df['Cluster'].isin(top_clusters)]\n",
        "\n",
        "    # Pivot the data to get years as rows and clusters as columns\n",
        "    pivot_table = top_clusters_df.groupby(['Year', 'Cluster']).size().unstack().fillna(0)\n",
        "\n",
        "    # Sort columns based on their total count across all years\n",
        "    sorted_columns = pivot_table.sum().sort_values(ascending=False).index\n",
        "    pivot_table = pivot_table[sorted_columns]\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    bottom = np.zeros(len(pivot_table))\n",
        "\n",
        "    for cluster in sorted_columns:\n",
        "        plt.bar(pivot_table.index, pivot_table[cluster], bottom=bottom, label=cluster)\n",
        "        bottom += pivot_table[cluster].values\n",
        "\n",
        "    plt.title('Yearly Counts of Top 10 AI Failure Descriptions Clusters')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xlabel('Year')\n",
        "    plt.xticks(pivot_table.index, rotation=45)  # Ensure each year is displayed\n",
        "    plt.gca().yaxis.grid(False)    # Remove the y-axis grid\n",
        "    plt.gca().xaxis.grid(False)    # Remove the x-axis grid\n",
        "    plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_clusters_stacked_bar_yearly(incident_dataset)"
      ],
      "metadata": {
        "id": "Ko6BJOr3p9in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F6T1DvxyBNLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import Dataset\n",
        "\n",
        "# # Convert the pandas dataframe to a Hugging Face dataset\n",
        "# hf_dataset = Dataset.from_pandas(incident_dataset)\n",
        "\n",
        "# # Push the dataset to the Hugging Face Hub\n",
        "# hf_dataset.push_to_hub(\n",
        "#     repo_id=\"vitaliy-sharandin/ai-incidents\"\n",
        "# )"
      ],
      "metadata": {
        "id": "6mUjkBe3qkbK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}