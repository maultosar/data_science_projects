{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitaliy-sharandin/data_science_projects/blob/master/portfolio/classification/Fraud_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets\n",
        "* https://www.kaggle.com/datasets/ealaxi/paysim1\n",
        "\n",
        "# Tutorial\n",
        "* https://thecleverprogrammer.com/2022/02/22/online-payments-fraud-detection-with-machine-learning/"
      ],
      "metadata": {
        "id": "UFwLbWGJELj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA"
      ],
      "metadata": {
        "id": "xmjDO-dXlFtg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RKHaB7am7E9R"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q datasets\n",
        "!pip install -U -q ydata-profiling\n",
        "!pip install -U -q feature_engine\n",
        "!pip install -U -q optuna\n",
        "!pip install -U -q boruta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from ydata_profiling import ProfileReport\n",
        "import pandas as pd\n",
        "from feature_engine.encoding import OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import make_scorer, fbeta_score\n",
        "import optuna\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from boruta import BorutaPy"
      ],
      "metadata": {
        "id": "LE06-7GPyXAW",
        "outputId": "745c9bca-2787-4999-8796-184aa1c685fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1c3cc8339865>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mydata_profiling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProfileReport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfeature_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrdinalEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.14.5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetCard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetCardData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHfFolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrequests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'DatasetCard' from 'huggingface_hub' (/usr/local/lib/python3.10/dist-packages/huggingface_hub/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_dataset = load_dataset(\"vitaliy-sharandin/synthetic-fraud-detection\")\n",
        "fraud_df = fraud_dataset['train'].to_pandas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "K7Ln-prYlI7D",
        "outputId": "d81b5f05-7dbc-4964-a0da-cc40b71c6999"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4d937a8e932e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfraud_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vitaliy-sharandin/synthetic-fraud-detection\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfraud_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfraud_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# profile = ProfileReport(fraud_df, title=\"Fraud data report\", dark_mode=True)\n",
        "# profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "zmMErdd7lf2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_df = fraud_df.drop(columns='isFlaggedFraud')"
      ],
      "metadata": {
        "id": "-nmQwawDt2Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df_length = 2000000\n",
        "\n",
        "fraud_samples = fraud_df[fraud_df['isFraud'] == 0]\n",
        "non_fraud_samples = fraud_df[fraud_df['isFraud'] == 1]\n",
        "\n",
        "fraud_samples_to_keep = new_df_length - len(non_fraud_samples)\n",
        "\n",
        "fraud_downsampled = fraud_samples.sample(n=fraud_samples_to_keep, random_state=42)\n",
        "\n",
        "df_downsampled = pd.concat([fraud_downsampled, non_fraud_samples], axis=0)\n",
        "\n",
        "fraud_df = df_downsampled.sample(frac=1, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "9aAT3bfyS3uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase results\n",
        "* Analysis\n",
        "  * No missing values.\n",
        "  * No feature is highly correlated with target.\n",
        "  * Some features are highly correlated among themselves.\n",
        "\n"
      ],
      "metadata": {
        "id": "6zBm_vx5xzBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature selection and engineering"
      ],
      "metadata": {
        "id": "LRnJMI-g03go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize_variables(target, df_train, cat_numeric_unique_threshold=10):\n",
        "  target = target\n",
        "  categorical_numeric = [var for var in df_train.columns if df_train[var].dtype!='O' and var!=target and df_train[var].nunique()<=cat_numeric_unique_threshold]\n",
        "  continuous = [var for var in df_train.columns if df_train[var].dtype!='O' and var!=target and var not in categorical_numeric]\n",
        "  mixed = [var for var in df_train.columns if pd.api.types.infer_dtype(df_train[var]) == 'mixed']\n",
        "  categorical_object = [var for var in df_train.columns if df_train[var].dtype=='O' and var not in mixed]\n",
        "  sorted_features = [target]+categorical_numeric+continuous+categorical_object+mixed\n",
        "  print('Total columns: '+str(df_train.columns.size)+'\\nColumns after sorting: '+str(len(sorted_features)))\n",
        "  return target, categorical_numeric, continuous, mixed, categorical_object\n",
        "target, categorical_numeric, continuous, mixed, categorical_object = categorize_variables('isFraud', fraud_df)"
      ],
      "metadata": {
        "id": "qgd7kRDp03C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(fraud_df[fraud_df.columns.difference([target])], fraud_df[target], test_size=0.2, stratify=fraud_df[target], random_state=42)"
      ],
      "metadata": {
        "id": "nQ4ITYBQxx_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OrdinalEncoder(\n",
        "    variables=categorical_object,\n",
        "    encoding_method='ordered'\n",
        ")\n",
        "\n",
        "X_train = encoder.fit_transform(X_train, y_train)\n",
        "X_test = encoder.fit_transform(X_test, y_test)"
      ],
      "metadata": {
        "id": "ACR0BcPR7YTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# smote = SMOTE(random_state=42)\n",
        "# X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "# X_test, y_test = smote.fit_resample(X_test, y_test)"
      ],
      "metadata": {
        "id": "fKvGVHXyA27s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# params = {\n",
        "#     'objective': 'binary:logistic',  # binary classification\n",
        "#     'eval_metric': 'logloss',        # log-likelihood loss\n",
        "#     'eta': 0.1,                      # learning rate\n",
        "#     'max_depth': 6,                  # maximum depth of a tree\n",
        "#     'subsample': 0.7,                # subsample ratio of the training instances\n",
        "#     'colsample_bytree': 0.7,          # subsample ratio of columns when constructing each tree .\n",
        "#     'tree_method': 'hist'\n",
        "# }\n",
        "\n",
        "# model = XGBClassifier(**params)\n",
        "\n",
        "# feat_selector = BorutaPy(model, n_estimators='auto', random_state=42)\n",
        "# feat_selector.fit(X_train.values, y_train.values)\n",
        "# selected_rf_features = pd.DataFrame({'Feature':list(X_train.columns),\n",
        "#                                        'Ranking':feat_selector.ranking_}).sort_values(by='Ranking')\n",
        "# selected_rf_features.nsmallest(40, 'Ranking').plot.barh(x='Feature',figsize=(24,5))"
      ],
      "metadata": {
        "id": "PSkZanMOYq7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model selection"
      ],
      "metadata": {
        "id": "UOi4x5Qp5mle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "\n",
        "    classifier_name = trial.suggest_categorical(\"classifier\", ['XGBoost', 'LGBM'])\n",
        "\n",
        "    if classifier_name ==\"XGBoost\":\n",
        "      params = {\n",
        "          # \"device\": 'cuda',\n",
        "          \"objective\": \"binary:logistic\",\n",
        "          \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
        "          \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
        "          \"max_depth\" : trial.suggest_int(\"max_depth\", 1, 9),\n",
        "          \"eta\" : trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
        "          \"gamma\" : trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
        "          \"grow_policy\" : trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
        "      }\n",
        "      model = XGBClassifier(**params)\n",
        "\n",
        "    else:\n",
        "      params = {\n",
        "          \"objective\": \"binary\",\n",
        "          \"metric\": \"binary_logloss\",\n",
        "          \"verbosity\": -1,\n",
        "          \"boosting_type\": \"gbdt\",\n",
        "          # \"device\" : \"gpu\",\n",
        "          \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
        "          \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
        "          \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
        "          \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
        "          \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
        "          \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
        "          \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
        "      }\n",
        "      model = LGBMClassifier(**params)\n",
        "\n",
        "    f2_scorer = make_scorer(fbeta_score, beta=2)\n",
        "    score = cross_val_score(model, X_train, y_train, cv=3, scoring=f2_scorer)\n",
        "    average_score = score.mean()\n",
        "\n",
        "    return average_score\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "display(study.best_params)\n",
        "display(study.best_value)"
      ],
      "metadata": {
        "id": "EhLGiJpM8QM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8G2EWec2yF8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}