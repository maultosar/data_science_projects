{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitaliy-sharandin/data_science_projects/blob/master/portfolio/nlp/fine-tuned-llm/psy_ai_wandb_tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8fa5101a-1278-47c5-948a-fe224c089686",
          "showTitle": false,
          "title": ""
        },
        "id": "_xQyhsoXyO0K"
      },
      "source": [
        "# Psy AI\n",
        "Psy AI is a Llama3-8b model instruction fine-tuned on depression dataset and is meant to help improve mental well-being.\n",
        "\n",
        "In future iterations it is meant to be trained on multiple philosophical and psychological datasets in order to provide multifaceted answers to complex mental health issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6b4773ad-485c-4b64-a100-3ed5866a25ec",
          "showTitle": false,
          "title": ""
        },
        "id": "zk7zN_7jYMZr"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1f0f94a8-3eef-4793-804c-7419d014cfd8",
          "showTitle": false,
          "title": ""
        },
        "id": "cZ-ecgabMSzo"
      },
      "source": [
        "Philosophy datasets (* for future training)\n",
        "* https://www.kaggle.com/datasets/christopherlemke/philosophical-texts\n",
        "* https://www.workwithdata.com/object/philosophy-science-complete-a-text-on-traditional-problems-schools-thought-book-by-edwin-h-c-hung-0000\n",
        "* https://www.kaggle.com/datasets/christopherlemke/philosophy-authors-writings-german\n",
        "* https://www.workwithdata.com/object/philosophical-inquiries-an-introduction-to-problems-philosophy-book-by-nicholas-rescher-0000\n",
        "* https://www.workwithdata.com/object/roman-stoicism-book-by-edward-vernon-arnold-1857\n",
        "* https://www.workwithdata.com/object/wisdom-energy-basic-buddhist-teachings-book-by-thubten-yeshe-1935"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "500d53d1-738f-4476-890f-63b6f145bf03",
          "showTitle": false,
          "title": ""
        },
        "id": "bkTCPgfTLIZh"
      },
      "source": [
        "Psychology and mental health datasets\n",
        "\n",
        "* Kaggle Depression data for chatbot https://www.kaggle.com/datasets/nupurgopali/depression-data-for-chatbot\n",
        "* Kaggle Psychometrics dataset https://www.kaggle.com/discussions/general/304994\n",
        "* Psychometric tests dataset https://ieee-dataport.org/documents/psychometric-tests-dataset\n",
        "* Psychometric NLP https://paperswithcode.com/dataset/psychometric-nlp\n",
        "* Reddit mental health dataset https://zenodo.org/record/3941387\n",
        "* Reddit mental disorders identification https://www.kaggle.com/datasets/kamaruladha/mental-disorders-identification-reddit-nlp\n",
        "* Kaggle Mental Health Conversational Data https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data\n",
        "* Kaggle Mental Health FAQ for Chatbot https://www.kaggle.com/narendrageek/mental-health-faq-for-chatbot/code\n",
        "* A human consciousness questionnaire dataset https://data.mendeley.com/datasets/69p62ksdh6\n",
        "* paperswithcode Self-reported Mental Health Diagnoses https://paperswithcode.com/dataset/smhd\n",
        "* paperswithcode Mental Health Summarization Dataset https://paperswithcode.com/dataset/mentsum\n",
        "* HuggingFace psychology dataset https://huggingface.co/datasets/samhog/psychology-10k\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "993e2c33-e4d5-4a21-8aa6-62ab49226721",
          "showTitle": false,
          "title": ""
        },
        "id": "bGXegQDzNtru"
      },
      "outputs": [],
      "source": [
        "# !pip install -U -q gradio\n",
        "# !pip install -U -q datasets\n",
        "# !pip install -U -q bitsandbytes\n",
        "# !pip install -q -U transformers\n",
        "# !pip install -q -U peft\n",
        "# !pip install -q -U accelerate\n",
        "# !pip install -U -q trl\n",
        "\n",
        "# !pip install -U -q evaluate\n",
        "# !pip install -U -q rouge_score\n",
        "# !pip install -U -q optuna\n",
        "# !pip install -U -q wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1ccc1208-e995-421e-a773-00f4b302f6cb",
          "showTitle": false,
          "title": ""
        },
        "id": "TKvdMbEluPVb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "import yaml\n",
        "import gradio as gr\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import GenerationConfig, Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel, AutoPeftModelForCausalLM\n",
        "import numpy as np\n",
        "from evaluate import load\n",
        "import optuna\n",
        "import wandb\n",
        "import datetime\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7427406b-3d7a-4556-87c8-ad836ad8fcb7",
          "showTitle": false,
          "title": ""
        },
        "id": "iiHYVFJyQuS2"
      },
      "source": [
        "# Dataset instruction transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "cbb2b618-5c57-400c-a2ed-6f0b7e60ff09",
          "showTitle": false,
          "title": ""
        },
        "id": "T-UWXlYqAErf"
      },
      "source": [
        "Depression dataset with 51 q&a entries was taken for mvp to see if model is indeed training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e62c7f46-6924-47a4-ab3a-2c107d73653d",
          "showTitle": false,
          "title": ""
        },
        "id": "LvA-W04yBvga"
      },
      "outputs": [],
      "source": [
        "depression_dataset = load_dataset(\"vitaliy-sharandin/depression-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "14709b86-cf55-4279-9756-7aed943255fe",
          "showTitle": false,
          "title": ""
        },
        "id": "fhM-wT3iAtm1"
      },
      "source": [
        "First, we modify our dataset to Alpaca format and create two datasets. One - for testing and evaluation and second one for inference, where responses are not available in formatted prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9690b50f-2d3e-48a3-bca8-b0b6604673cd",
          "showTitle": false,
          "title": ""
        },
        "id": "hwGaZ5y5A53T"
      },
      "outputs": [],
      "source": [
        "def formatting_func_train(example):\n",
        "  if example.get(\"context\", \"\") != \"\":\n",
        "      input_prompt = (f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Input: \\n\"\n",
        "      f\"{example['context']}\\n\\n\"\n",
        "      f\"### Response: \\n\"\n",
        "      f\"{example['response']}\")\n",
        "\n",
        "  else:\n",
        "    input_prompt = (f\"Below is an instruction that describes a task. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Response:\\n\"\n",
        "      f\"{example['response']}\")\n",
        "\n",
        "  return {\"text\" : input_prompt}\n",
        "\n",
        "def formatting_func_test(example):\n",
        "  if example.get(\"context\", \"\") != \"\":\n",
        "      input_prompt = (f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Input: \\n\"\n",
        "      f\"{example['context']}\\n\\n\"\n",
        "      f\"### Response: \\n\")\n",
        "\n",
        "  else:\n",
        "    input_prompt = (f\"Below is an instruction that describes a task. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Response: \\n\")\n",
        "\n",
        "  return {\"text\" : input_prompt}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e4c7f4e9-04d8-4cdb-8a69-ddc81a30589c",
          "showTitle": false,
          "title": ""
        },
        "id": "riRBYQdGA7Mc"
      },
      "outputs": [],
      "source": [
        "formatted_depression_dataset_train = depression_dataset['train'].map(formatting_func_train)\n",
        "formatted_depression_dataset_test = depression_dataset['train'].map(formatting_func_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7c826500-d0c1-4710-ac72-3bc4caf88540",
          "showTitle": false,
          "title": ""
        },
        "id": "-JWMiV2MM98j"
      },
      "source": [
        "# Model load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "33d01c00-365a-41b2-af4f-2cece650074c",
          "showTitle": false,
          "title": ""
        },
        "id": "Y1PfT1wI42ky"
      },
      "source": [
        "We are loading our Llama2 model in 4bit quantized form as well as applying Lora for peft training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c6adb738-73d7-40c4-87c2-397ccad6211a",
          "showTitle": false,
          "title": ""
        },
        "id": "ysK6WLI_qs6m"
      },
      "outputs": [],
      "source": [
        "model_name = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "def get_tokenizer():\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  tokenizer.padding_side = \"right\"\n",
        "  return tokenizer\n",
        "\n",
        "def get_model():\n",
        "\n",
        "\n",
        "  bnb_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_quant_type=\"nf4\",\n",
        "      bnb_4bit_compute_dtype=torch.bfloat16\n",
        "  )\n",
        "\n",
        "  qlora_config = LoraConfig(lora_alpha=16,\n",
        "                          lora_dropout=0.1,\n",
        "                          r=64,\n",
        "                          bias=\"none\",\n",
        "                          task_type=\"CAUSAL_LM\")\n",
        "\n",
        "  base_training_model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      quantization_config=bnb_config,\n",
        "      device_map = {\"\": 0}\n",
        "  )\n",
        "\n",
        "  base_training_model = prepare_model_for_kbit_training(base_training_model)\n",
        "  base_training_model = get_peft_model(base_training_model, qlora_config)\n",
        "  return base_training_model.to('cuda')\n",
        "\n",
        "base_training_model = get_model()\n",
        "tokenizer = get_tokenizer()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "print(base_training_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e2592187-511e-4d5f-a338-9c8c796e1e56",
          "showTitle": false,
          "title": ""
        },
        "id": "C8z9BjCdYPuB"
      },
      "source": [
        "# Model instruction fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f76a6d93-cba3-4b7e-bfe6-e25aad98d96a",
          "showTitle": false,
          "title": ""
        },
        "id": "mzN1Xpmp21BG"
      },
      "source": [
        "Here we are defining inference function which returns bleu, rouge and f1 metrics after comparison of predicted and reference responses. My tests have shown that standard trainer `compute_metrics` method is quite inefficient and is not quite suitable for instruction fine-tuning during manual observations or generated results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "022438a6-8335-451d-ab12-885a2c9436e9",
          "showTitle": false,
          "title": ""
        },
        "id": "IsGjKseN3XFh"
      },
      "outputs": [],
      "source": [
        "entity=\"vitaliy-sharandin-org\"\n",
        "project_name=\"Psy-AI\"\n",
        "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
        "\n",
        "model_trained_checkpoint = 'llama3-trained'\n",
        "registry_model_name = \"psy-ai-model\"\n",
        "\n",
        "def bleu_rouge_f1_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "  labels = [[idx for idx in label if idx != -100] for label in labels]\n",
        "  predictions = [[idx for idx in prediction if idx != -100] for prediction in predictions]\n",
        "\n",
        "  decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  bleu = load(\"bleu\")\n",
        "  bleu_results = bleu.compute(predictions=decoded_predictions, references=decoded_labels)\n",
        "\n",
        "  rouge = load('rouge')\n",
        "  rouge_results = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n",
        "\n",
        "  f1 = 2 * (bleu_results['bleu'] * rouge_results['rouge1']) / (bleu_results['bleu'] + rouge_results['rouge1'])\n",
        "\n",
        "  scores = {\n",
        "        \"bleu\": bleu_results[\"bleu\"],\n",
        "        \"rouge1\": rouge_results[\"rouge1\"],\n",
        "        \"rouge2\": rouge_results[\"rouge2\"],\n",
        "        \"rougeL\": rouge_results[\"rougeL\"],\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "  wandb.log(scores)\n",
        "\n",
        "  return scores\n",
        "\n",
        "def fine_tune(model, tokenizer, train_dataset, eval_dataset, metrics_func, only_evaluate=False):\n",
        "\n",
        "  supervised_finetuning_trainer = SFTTrainer(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            eval_dataset=eval_dataset,\n",
        "                                            args=TrainingArguments(\n",
        "                                                output_dir=\"./training_results\",\n",
        "                                                report_to=\"wandb\",\n",
        "                                                per_device_train_batch_size=8,\n",
        "                                                per_device_eval_batch_size=1,\n",
        "                                                gradient_accumulation_steps=2,\n",
        "                                                ddp_find_unused_parameters=False,\n",
        "                                                optim=\"paged_adamw_8bit\",\n",
        "                                                save_steps=1000,\n",
        "                                                logging_steps=30,\n",
        "                                                learning_rate=2e-4,\n",
        "                                                weight_decay=0.001,\n",
        "                                                fp16=False,\n",
        "                                                bf16=False,\n",
        "                                                max_grad_norm=0.3,\n",
        "                                                max_steps=-1,\n",
        "                                                warmup_ratio=0.3,\n",
        "                                                group_by_length=True,\n",
        "                                                lr_scheduler_type=\"constant\"\n",
        "                                            ),\n",
        "                                            dataset_text_field=\"text\",\n",
        "                                            max_seq_length=2048,\n",
        "                                            compute_metrics=metrics_func,\n",
        "                                            data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False))\n",
        "\n",
        "  if only_evaluate:\n",
        "    return supervised_finetuning_trainer.evaluate()\n",
        "\n",
        "  else:\n",
        "    with wandb.init(entity=entity, project=project_name) as run:\n",
        "      supervised_finetuning_trainer.train()\n",
        "      supervised_finetuning_trainer.model.save_pretrained(model_trained_checkpoint)\n",
        "      tokenizer.save_pretrained(model_trained_checkpoint)\n",
        "      run.link_model(path=model_trained_checkpoint, registered_model_name={entity}/{project_name}/{registry_model_name})\n",
        "\n",
        "      wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d6b57539-d1c2-497d-a1dc-b8401c480792",
          "showTitle": false,
          "title": ""
        },
        "id": "irSVYaRU4op0"
      },
      "source": [
        "## Evaluating base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "65e93ddb-c8c5-4744-9188-9f4231580f5d",
          "showTitle": false,
          "title": ""
        },
        "id": "NBhgMjWAE_6S"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"model\": base_training_model,\n",
        "    \"tokenizer\": tokenizer,\n",
        "    \"train_dataset\": formatted_depression_dataset_train,\n",
        "    \"eval_dataset\": formatted_depression_dataset_train,\n",
        "    \"metrics_func\": bleu_rouge_f1_metrics,\n",
        "    \"only_evaluate\": True\n",
        "}\n",
        "\n",
        "fine_tune(**params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2de1a8c3-6a52-4ddd-a484-1a385fa79bb5",
          "showTitle": false,
          "title": ""
        },
        "id": "e199cTOsNrgV"
      },
      "source": [
        "Evaluation metrics are quite low before training.\n",
        "\n",
        "Let's train the model and see how metrics and responses change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "25219d74-a6c5-4d2e-8874-06a8e89138c8",
          "showTitle": false,
          "title": ""
        },
        "id": "ernG2FcS4tuG"
      },
      "source": [
        "## Fine-tuning base model and registering it to WanDB model registry\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "adf01b50-df15-48b8-8186-7e40783aca54",
          "showTitle": false,
          "title": ""
        },
        "id": "Z8RswZBWxqfR"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"model\": base_training_model,\n",
        "    \"tokenizer\": tokenizer,\n",
        "    \"train_dataset\": formatted_depression_dataset_train,\n",
        "    \"eval_dataset\": formatted_depression_dataset_train,\n",
        "    \"metrics_func\": bleu_rouge_f1_metrics\n",
        "}\n",
        "\n",
        "fine_tune(**params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "342f7272-6626-4d53-af94-cb67fd845693",
          "showTitle": false,
          "title": ""
        },
        "id": "x0agS0CmSh6a"
      },
      "source": [
        "## Loading and evaluating model from Wandb model registry"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init()\n",
        "downloaded_model_path = run.use_model(name=f\"{entity}/{project_name}/{registry_model_name}:latest\")"
      ],
      "metadata": {
        "id": "WjXSeKzQUuJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(downloaded_model_path, device_map=\"auto\", quantization_config=BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_quant_type=\"nf4\",\n",
        "      bnb_4bit_compute_dtype=torch.bfloat16\n",
        "  ))"
      ],
      "metadata": {
        "id": "NRBYVqR_cQr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"model\": model,\n",
        "    \"tokenizer\": tokenizer,\n",
        "    \"train_dataset\": formatted_depression_dataset_train,\n",
        "    \"eval_dataset\": formatted_depression_dataset_train,\n",
        "    \"metrics_func\": bleu_rouge_f1_metrics,\n",
        "    \"only_evaluate\": True\n",
        "}\n",
        "\n",
        "fine_tune(**params)"
      ],
      "metadata": {
        "id": "eqJJRhOqfJxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5e7e87ec-ba40-4a48-bfee-d8fd6dda6111",
          "showTitle": false,
          "title": ""
        },
        "id": "C9agpWsaZUL0"
      },
      "source": [
        "# Saving fine-tuned model with adapters to Huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "45d02fc0-076f-4371-83d6-bd8d86f3c400",
          "showTitle": false,
          "title": ""
        },
        "id": "baJM5WLITraq"
      },
      "source": [
        "Now we are able to merge our model with saved peft adapters and push it to Hugging Face repo.\n",
        "\n",
        "We have to launch this cell separately, as VRAM will be filled up to fullest point when we merge adapters with original model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "04708f94-9b04-4b2a-9703-79f855067ce9",
          "showTitle": false,
          "title": ""
        },
        "id": "20saoYRljDzV"
      },
      "outputs": [],
      "source": [
        "# tuned_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "#     model_trained_checkpoint,\n",
        "#     low_cpu_mem_usage=True,\n",
        "#     torch_dtype=torch.float16,\n",
        "#     device_map = {\"\": 0}\n",
        "# )\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_trained_checkpoint)\n",
        "\n",
        "# merged_model = tuned_model.merge_and_unload()\n",
        "\n",
        "# merged_model.save_pretrained(model_merged, safe_serialization=True)\n",
        "# tokenizer.save_pretrained(model_merged)\n",
        "\n",
        "# mlflow.register_model(model_uri=model_merged, name=\"psy-ai\")\n",
        "\n",
        "# # token=''\n",
        "# # merged_model.push_to_hub(\"vitaliy-sharandin/wiseai\", token=token)\n",
        "# # tokenizer.push_to_hub(\"vitaliy-sharandin/wiseai\", token=token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "aed40ac0-70f1-460a-a97c-1ba0e689f90b",
          "showTitle": false,
          "title": ""
        },
        "id": "afAivp4AEmgr"
      },
      "source": [
        "# Gradio demo in Hugging Face Spaces\n",
        "\n",
        "https://huggingface.co/spaces/vitaliy-sharandin/WiseAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "cc797601-6ed6-435e-aea0-7f82acfc1e24",
          "showTitle": false,
          "title": ""
        },
        "id": "L1kfrRdm6XDS"
      },
      "source": [
        "## Local launch\n",
        "Requirements:\n",
        "* Choose free T4 GPU in Colab environment settings\n",
        "* Run `pip install` and `imports` cells at the beginning before launching Gradio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8f1b143c-8214-4b1f-99ba-bc655efe2f4a",
          "showTitle": false,
          "title": ""
        },
        "id": "TbFojGP4s2VZ"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "      'vitaliy-sharandin/wiseai',\n",
        "      load_in_8bit=True,\n",
        "      device_map = {\"\": 0}\n",
        "  )\n",
        "tokenizer = AutoTokenizer.from_pretrained('vitaliy-sharandin/wiseai')\n",
        "\n",
        "pipe = pipeline('text-generation', model=model,tokenizer=tokenizer)\n",
        "\n",
        "def generate_text(instruction, input):\n",
        "  if not instruction.strip():\n",
        "    return str('The instruction field is required.')\n",
        "\n",
        "  if instruction.strip() and input.strip():\n",
        "    input_prompt = (f\"Below is an instruction that describes a task. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{instruction}\\n\\n\"\n",
        "      \"### Input:\\n\"\n",
        "      f\"{input}\\n\\n\"\n",
        "      f\"### Response: \\n\")\n",
        "  else :\n",
        "    input_prompt = (f\"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n\"\n",
        "        f\"{instruction}\\n\\n\"\n",
        "        f\"### Response: \\n\")\n",
        "  result = pipe(input_prompt, max_length=200, top_p=0.9, temperature=0.9, num_return_sequences=1, return_full_text=False)[0]['generated_text']\n",
        "  return result[:str(result).find(\"###\")]\n",
        "\n",
        "iface = gr.Interface(fn=generate_text, inputs=[gr.Textbox(label=\"Instruction\"),\n",
        "                                               gr.Textbox(label=\"Additional Input\")],\n",
        "                     outputs=gr.Textbox(label=\"Response\"))\n",
        "iface.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "psy_ai_mlflow_tracking_deployment",
      "widgets": {}
    },
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}