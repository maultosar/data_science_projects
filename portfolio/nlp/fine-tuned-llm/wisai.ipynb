{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitaliy-sharandin/data_science_projects/blob/master/portfolio/nlp/fine-tuned-llm/wisai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WisAI\n",
        "### WisAI model is a GPT-NeoX-20B model fine-tuned on philosophical and psychological data and configured to provide useful advice.\n",
        "\n"
      ],
      "metadata": {
        "id": "_xQyhsoXyO0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "zk7zN_7jYMZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Philosophy datasets\n",
        "* https://www.kaggle.com/datasets/christopherlemke/philosophical-texts\n",
        "* https://www.workwithdata.com/object/philosophy-science-complete-a-text-on-traditional-problems-schools-thought-book-by-edwin-h-c-hung-0000\n",
        "* https://www.kaggle.com/datasets/christopherlemke/philosophy-authors-writings-german\n",
        "* https://www.workwithdata.com/object/philosophical-inquiries-an-introduction-to-problems-philosophy-book-by-nicholas-rescher-0000\n",
        "* https://www.workwithdata.com/object/roman-stoicism-book-by-edward-vernon-arnold-1857\n",
        "* https://www.workwithdata.com/object/wisdom-energy-basic-buddhist-teachings-book-by-thubten-yeshe-1935"
      ],
      "metadata": {
        "id": "cZ-ecgabMSzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Psychology and mental health datasets\n",
        "\n",
        "#### Text datasets\n",
        "\n",
        "\n",
        "* Kaggle Psychometrics dataset https://www.kaggle.com/discussions/general/304994\n",
        "* Psychometric tests dataset https://ieee-dataport.org/documents/psychometric-tests-dataset\n",
        "* Psychometric NLP https://paperswithcode.com/dataset/psychometric-nlp\n",
        "* Reddit mental health dataset https://zenodo.org/record/3941387\n",
        "* Reddit mental disorders identification https://www.kaggle.com/datasets/kamaruladha/mental-disorders-identification-reddit-nlp\n",
        "* Kaggle Mental Health Conversational Data https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data\n",
        "* Kaggle Mental Health FAQ for Chatbot https://www.kaggle.com/narendrageek/mental-health-faq-for-chatbot/code\n",
        "* A human consciousness questionnaire dataset https://data.mendeley.com/datasets/69p62ksdh6\n",
        "* paperswithcode Self-reported Mental Health Diagnoses https://paperswithcode.com/dataset/smhd\n",
        "* paperswithcode Mental Health Summarization Dataset https://paperswithcode.com/dataset/mentsum\n",
        "* HuggingFace psychology dataset https://huggingface.co/datasets/samhog/psychology-10k\n",
        "\n",
        "#### Text2Text datasets\n",
        "* Kaggle Depression data for chatbot https://www.kaggle.com/datasets/nupurgopali/depression-data-for-chatbot\n",
        "\n",
        "#### Classification datasets\n",
        "* Classification for mental health https://www.kaggle.com/datasets/reihanenamdari/mental-health-corpus\n",
        "* Depression identification https://www.kaggle.com/datasets/infamouscoder/depression-reddit-cleaned\n",
        "\n",
        "## Tutorials\n",
        "https://www.philschmid.de/instruction-tune-llama-2"
      ],
      "metadata": {
        "id": "bkTCPgfTLIZh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.8/395.8 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.0/124.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q gradio\n",
        "!pip install -U -q datasets\n",
        "!pip install -U -q bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -U -q trl\n",
        "\n",
        "!pip install -U -q evaluate\n",
        "!pip install -U -q rouge_score"
      ],
      "metadata": {
        "id": "bGXegQDzNtru",
        "outputId": "70686394-029b-47a1-df49-0e36f99c7672",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "import yaml\n",
        "import gradio as gr\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import GenerationConfig, Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel, AutoPeftModelForCausalLM\n",
        "import numpy as np\n",
        "from evaluate import load\n",
        "import optuna\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "TKvdMbEluPVb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset instruction transformation"
      ],
      "metadata": {
        "id": "iiHYVFJyQuS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depression_dataset = load_dataset(\"vitaliy-sharandin/depression-instruct\")"
      ],
      "metadata": {
        "id": "LvA-W04yBvga"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(examples):\n",
        "  output_texts = []\n",
        "  for i in range(len(examples['instruction'])):\n",
        "    if examples.get(\"context\", \"\") != \"\":\n",
        "        input_prompt = (f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n\"\n",
        "        f\"{examples['instruction'][i]}\\n\\n\"\n",
        "        f\"### Input: \\n\"\n",
        "        f\"{examples['context'][i]}\\n\\n\"\n",
        "        f\"### Response: \\n\"\n",
        "        f\"{examples['response'][i]}\")\n",
        "\n",
        "    else:\n",
        "      input_prompt = (f\"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n\"\n",
        "        f\"{examples['instruction'][i]}\\n\\n\"\n",
        "        f\"### Response:\\n\"\n",
        "        f\"{examples['response'][i]}\")\n",
        "\n",
        "    output_texts.append(input_prompt)\n",
        "\n",
        "  return output_texts\n",
        "\n",
        "def formatting_func_train(example):\n",
        "  if example.get(\"context\", \"\") != \"\":\n",
        "      input_prompt = (f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Input: \\n\"\n",
        "      f\"{example['context']}\\n\\n\"\n",
        "      f\"### Response: \\n\"\n",
        "      f\"{example['response']}\")\n",
        "\n",
        "  else:\n",
        "    input_prompt = (f\"Below is an instruction that describes a task. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Response:\\n\"\n",
        "      f\"{example['response']}\")\n",
        "\n",
        "  return {\"text\" : input_prompt}\n",
        "\n",
        "def formatting_func_test(example):\n",
        "  if example.get(\"context\", \"\") != \"\":\n",
        "      input_prompt = (f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Input: \\n\"\n",
        "      f\"{example['context']}\\n\\n\"\n",
        "      f\"### Response: \\n\")\n",
        "\n",
        "  else:\n",
        "    input_prompt = (f\"Below is an instruction that describes a task. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Response: \\n\")\n",
        "\n",
        "  return {\"text\" : input_prompt}"
      ],
      "metadata": {
        "id": "hwGaZ5y5A53T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_depression_dataset_train = depression_dataset.map(formatting_func_train)\n",
        "formatted_depression_dataset_test = depression_dataset.map(formatting_func_test)"
      ],
      "metadata": {
        "id": "riRBYQdGA7Mc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model load"
      ],
      "metadata": {
        "id": "-JWMiV2MM98j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer():\n",
        "  model_name = \"NousResearch/Llama-2-7b-hf\"\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  tokenizer.padding_side = \"right\"\n",
        "  return tokenizer\n",
        "\n",
        "def get_model():\n",
        "  model_name = \"NousResearch/Llama-2-7b-hf\"\n",
        "\n",
        "  bnb_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_quant_type=\"nf4\",\n",
        "      bnb_4bit_compute_dtype=torch.bfloat16\n",
        "  )\n",
        "\n",
        "  qlora_config = LoraConfig(lora_alpha=16,\n",
        "                          lora_dropout=0.1,\n",
        "                          r=64,\n",
        "                          bias=\"none\",\n",
        "                          task_type=\"CAUSAL_LM\")\n",
        "\n",
        "  base_training_model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      quantization_config=bnb_config,\n",
        "      device_map = {\"\": 0}\n",
        "  )\n",
        "\n",
        "  base_training_model = prepare_model_for_kbit_training(base_training_model)\n",
        "  base_training_model = get_peft_model(base_training_model, qlora_config)\n",
        "  return base_training_model.to('cuda')\n",
        "\n",
        "base_training_model = get_model()\n",
        "tokenizer = get_tokenizer()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "print(base_training_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysK6WLI_qs6m",
        "outputId": "e4c797e5-dcc8-4e76-d0d6-0205d4f7a9f4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): LlamaForCausalLM(\n",
            "      (model): LlamaModel(\n",
            "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x LlamaDecoderLayer(\n",
            "            (self_attn): LlamaAttention(\n",
            "              (q_proj): Linear4bit(\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "              )\n",
            "              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "              (v_proj): Linear4bit(\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "              )\n",
            "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "              (rotary_emb): LlamaRotaryEmbedding()\n",
            "            )\n",
            "            (mlp): LlamaMLP(\n",
            "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
            "              (act_fn): SiLUActivation()\n",
            "            )\n",
            "            (input_layernorm): LlamaRMSNorm()\n",
            "            (post_attention_layernorm): LlamaRMSNorm()\n",
            "          )\n",
            "        )\n",
            "        (norm): LlamaRMSNorm()\n",
            "      )\n",
            "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model instruction fine-tuning"
      ],
      "metadata": {
        "id": "C8z9BjCdYPuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_trained_checkpoint = 'model-trained-checkpoint'\n",
        "model_merged = 'model-merged'\n",
        "\n",
        "def run_inference(model, inputs, reference_responses):\n",
        "\n",
        "  decoded_predictions = []\n",
        "  for prompt in inputs:\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    prediction = pipe(prompt, max_length=200, top_p=0.9, temperature=0.9, num_return_sequences=1, return_full_text=False)[0]['generated_text']\n",
        "    decoded_predictions.append(prediction[:str(prediction).find(\"###\")])\n",
        "\n",
        "  for input, pred, label in zip(inputs[:3], decoded_predictions[:3], reference_responses[:3]):\n",
        "    print(\"[Input]:\\n\\n\", input)\n",
        "    print(\"[Prediction]:\\n\\n\", pred)\n",
        "    print(\"[Reference response]:\\n\\n\", label)\n",
        "    print(\"----\")\n",
        "\n",
        "  bleu = load(\"bleu\")\n",
        "  bleu_results = bleu.compute(predictions=decoded_predictions, references=reference_responses)\n",
        "\n",
        "  rouge = load('rouge')\n",
        "  rouge_results = rouge.compute(predictions=decoded_predictions, references=reference_responses)\n",
        "\n",
        "  f1 = 2 * (bleu_results['bleu'] * rouge_results['rouge1']) / (bleu_results['bleu'] + rouge_results['rouge1'])\n",
        "\n",
        "  scores = {\n",
        "        \"bleu\": bleu_results[\"bleu\"],\n",
        "        \"rouge1\": rouge_results[\"rouge1\"],\n",
        "        \"rouge2\": rouge_results[\"rouge2\"],\n",
        "        \"rougeL\": rouge_results[\"rougeL\"],\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "  return scores\n",
        "\n",
        "\n",
        "def bleu_rouge_f1_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "  labels = [[idx for idx in label if idx != -100] for label in labels]\n",
        "  predictions = [[idx for idx in prediction if idx != -100] for prediction in predictions]\n",
        "\n",
        "  decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  for pred, label in zip(decoded_predictions[:3], decoded_labels[:3]):\n",
        "    print(\"[Prediction]:\\n\\n\", pred)\n",
        "    print(\"[Reference response]:\\n\\n\", label)\n",
        "    print(\"----\")\n",
        "\n",
        "  bleu = load(\"bleu\")\n",
        "  bleu_results = bleu.compute(predictions=decoded_predictions, references=decoded_labels)\n",
        "\n",
        "  rouge = load('rouge')\n",
        "  rouge_results = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n",
        "\n",
        "  f1 = 2 * (bleu_results['bleu'] * rouge_results['rouge1']) / (bleu_results['bleu'] + rouge_results['rouge1'])\n",
        "\n",
        "  scores = {\n",
        "        \"bleu\": bleu_results[\"bleu\"],\n",
        "        \"rouge1\": rouge_results[\"rouge1\"],\n",
        "        \"rouge2\": rouge_results[\"rouge2\"],\n",
        "        \"rougeL\": rouge_results[\"rougeL\"],\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "  return scores\n",
        "\n",
        "\n",
        "def fine_tune(training_model, tokenizer, train_dataset, eval_dataset, test_dataset, model_init_func, metrics_func, inference_func, only_evaluate=False, only_inference=False, hyper_opt=False, trained_inference=False):\n",
        "\n",
        "  reference_responses = test_dataset['response']\n",
        "\n",
        "  training_args = {\n",
        "        \"output_dir\": \"./training_results\",\n",
        "        \"num_train_epochs\": 10,\n",
        "        \"per_device_train_batch_size\": 8,\n",
        "        \"gradient_accumulation_steps\": 2,\n",
        "        \"optim\": \"paged_adamw_8bit\",\n",
        "        \"save_steps\": 1000,\n",
        "        \"logging_steps\": 30,\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"weight_decay\": 0.001,\n",
        "        \"fp16\": False,\n",
        "        \"bf16\": False,\n",
        "        \"max_grad_norm\": 0.3,\n",
        "        \"max_steps\": -1,\n",
        "        \"warmup_ratio\": 0.3,\n",
        "        \"group_by_length\": True,\n",
        "        \"lr_scheduler_type\": \"constant\"\n",
        "    }\n",
        "\n",
        "  supervised_finetuning_trainer = SFTTrainer(model=training_model,\n",
        "                                            model_init=model_init_func,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            eval_dataset=eval_dataset,\n",
        "                                            args=TrainingArguments(**training_args),\n",
        "                                            dataset_text_field=\"text\",\n",
        "                                            max_seq_length=2048,\n",
        "                                            compute_metrics=metrics_func,\n",
        "                                            data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False))\n",
        "\n",
        "  if only_evaluate:\n",
        "    return supervised_finetuning_trainer.evaluate()\n",
        "\n",
        "  elif only_inference:\n",
        "    return inference_func(supervised_finetuning_trainer.model, test_dataset['text'], reference_responses)\n",
        "\n",
        "  elif hyper_opt:\n",
        "    hyper_args = lambda trial: {\n",
        "          \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 25),\n",
        "          \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16]),\n",
        "          \"gradient_accumulation_steps\": trial.suggest_int(\"gradient_accumulation_steps\", 1, 5),\n",
        "          \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
        "          \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True),\n",
        "          \"max_grad_norm\": trial.suggest_float(\"max_grad_norm\", 0.1, 0.5),\n",
        "          \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.1, 0.5)\n",
        "      }\n",
        "    return supervised_finetuning_trainer.hyperparameter_search(direction=[\"maximize\"],\n",
        "                                                              backend=\"optuna\",\n",
        "                                                              hp_space=hyper_args,\n",
        "                                                              n_trials=20,\n",
        "                                                              compute_objective=lambda metrics: metrics['eval_f1'])\n",
        "\n",
        "  else:\n",
        "    supervised_finetuning_trainer.train()\n",
        "    supervised_finetuning_trainer.model.save_pretrained(model_trained_checkpoint)\n",
        "    tokenizer.save_pretrained(model_trained_checkpoint)\n",
        "\n",
        "    if trained_inference:\n",
        "      return inference_func(supervised_finetuning_trainer.model, test_dataset['text'], reference_responses)\n",
        "    else:\n",
        "      return supervised_finetuning_trainer.evaluate()"
      ],
      "metadata": {
        "id": "IsGjKseN3XFh"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"training_model\": base_training_model,\n",
        "    \"tokenizer\": tokenizer,\n",
        "    \"train_dataset\": formatted_depression_dataset_train[\"train\"].select(range(3)),\n",
        "    \"eval_dataset\": formatted_depression_dataset_train[\"train\"].select(range(3)),\n",
        "    \"test_dataset\": formatted_depression_dataset_test[\"train\"].select(range(3)),\n",
        "    \"model_init_func\": get_model,\n",
        "    \"metrics_func\": bleu_rouge_f1_metrics,\n",
        "    \"inference_func\": run_inference,\n",
        "    \"only_evaluate\": False,\n",
        "    \"only_inference\": False,\n",
        "    \"hyper_opt\": True,\n",
        "    \"trained_inference\": False\n",
        "}\n",
        "\n",
        "fine_tune(**params)"
      ],
      "metadata": {
        "id": "NBhgMjWAE_6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating base model"
      ],
      "metadata": {
        "id": "irSVYaRU4op0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scores_untrained_model = fine_tune(base_training_model, tokenizer, formatted_depression_dataset_train, formatted_depression_dataset_eval, bleu_rouge_f1, only_evaluate=True)\n",
        "# scores_untrained_model"
      ],
      "metadata": {
        "id": "9pcvtQXyPXwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning and evaluating model"
      ],
      "metadata": {
        "id": "ernG2FcS4tuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scores_untrained_model = fine_tune(base_training_model, tokenizer, formatted_depression_dataset_train, formatted_depression_dataset_eval, bleu_rouge_f1)\n",
        "# scores_untrained_model"
      ],
      "metadata": {
        "id": "Gwei4jMTY5Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Experiments\n",
        "1. Compare trained / untrained / small model results\n",
        "2. Complete training on all datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "5pGf2qH08XL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tuned_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "#     model_trained_checkpoint,\n",
        "#     low_cpu_mem_usage=True,\n",
        "#     torch_dtype=torch.float16,\n",
        "#     device_map = {\"\": 0}\n",
        "# )\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_merged)\n",
        "\n",
        "# merged_model = tuned_model.merge_and_unload()\n",
        "\n",
        "# merged_model.save_pretrained(model_merged, safe_serialization=True)\n",
        "# tokenizer.save_pretrained(model_merged)\n",
        "\n",
        "# token = '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'\n",
        "# merged_model.push_to_hub(\"vitaliy-sharandin/wisai\", token=token)\n",
        "# tokenizer.push_to_hub(\"vitaliy-sharandin/wisai\", token=token)"
      ],
      "metadata": {
        "id": "20saoYRljDzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot lauch"
      ],
      "metadata": {
        "id": "Sc-PH7n8Y8un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gen_config = GenerationConfig(\n",
        "#     do_sample=True,\n",
        "#     temperature=0.9,\n",
        "#     max_new_tokens=150,\n",
        "#     pad_token_id=tokenizer.eos_token_id,\n",
        "#     num_return_sequences=1\n",
        "# )\n",
        "\n",
        "# def predict(prompt):\n",
        "#     encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "#     input_length = len(encoded_input[\"input_ids\"][0])\n",
        "#     output_ids = model.generate(generation_config=gen_config, **encoded_input)[0]\n",
        "#     output = tokenizer.decode(output_ids[input_length:], skip_special_tokens=True)\n",
        "#     return output\n",
        "\n",
        "# #gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\").launch()\n",
        "# print(predict(\"What is Depression?\"))"
      ],
      "metadata": {
        "id": "Fb_grdNpOarF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving model components to Huggingface"
      ],
      "metadata": {
        "id": "C9agpWsaZUL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# token = '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'\n",
        "# model.push_to_hub(\"wisai\", use_auth_token=token)\n",
        "# gen_config.push_to_hub(\"wisai\", \"generation_config.json\", use_auth_token=token)"
      ],
      "metadata": {
        "id": "i5vnd2GgwlkQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}