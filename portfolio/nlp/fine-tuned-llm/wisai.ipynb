{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitaliy-sharandin/data_science_projects/blob/master/portfolio/nlp/fine-tuned-llm/wisai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WisAI\n",
        "### WisAI model is a GPT-NeoX-20B model fine-tuned on philosophical and psychological data and configured to provide useful advice.\n",
        "\n"
      ],
      "metadata": {
        "id": "_xQyhsoXyO0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "zk7zN_7jYMZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Philosophy datasets\n",
        "* https://www.kaggle.com/datasets/christopherlemke/philosophical-texts\n",
        "* https://www.workwithdata.com/object/philosophy-science-complete-a-text-on-traditional-problems-schools-thought-book-by-edwin-h-c-hung-0000\n",
        "* https://www.kaggle.com/datasets/christopherlemke/philosophy-authors-writings-german\n",
        "* https://www.workwithdata.com/object/philosophical-inquiries-an-introduction-to-problems-philosophy-book-by-nicholas-rescher-0000\n",
        "* https://www.workwithdata.com/object/roman-stoicism-book-by-edward-vernon-arnold-1857\n",
        "* https://www.workwithdata.com/object/wisdom-energy-basic-buddhist-teachings-book-by-thubten-yeshe-1935"
      ],
      "metadata": {
        "id": "cZ-ecgabMSzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Psychology and mental health datasets\n",
        "\n",
        "#### Text datasets\n",
        "\n",
        "\n",
        "* Kaggle Psychometrics dataset https://www.kaggle.com/discussions/general/304994\n",
        "* Psychometric tests dataset https://ieee-dataport.org/documents/psychometric-tests-dataset\n",
        "* Psychometric NLP https://paperswithcode.com/dataset/psychometric-nlp\n",
        "* Reddit mental health dataset https://zenodo.org/record/3941387\n",
        "* Reddit mental disorders identification https://www.kaggle.com/datasets/kamaruladha/mental-disorders-identification-reddit-nlp\n",
        "* Kaggle Mental Health Conversational Data https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data\n",
        "* Kaggle Mental Health FAQ for Chatbot https://www.kaggle.com/narendrageek/mental-health-faq-for-chatbot/code\n",
        "* A human consciousness questionnaire dataset https://data.mendeley.com/datasets/69p62ksdh6\n",
        "* paperswithcode Self-reported Mental Health Diagnoses https://paperswithcode.com/dataset/smhd\n",
        "* paperswithcode Mental Health Summarization Dataset https://paperswithcode.com/dataset/mentsum\n",
        "* HuggingFace psychology dataset https://huggingface.co/datasets/samhog/psychology-10k\n",
        "\n",
        "#### Text2Text datasets\n",
        "* Kaggle Depression data for chatbot https://www.kaggle.com/datasets/nupurgopali/depression-data-for-chatbot\n",
        "\n",
        "#### Classification datasets\n",
        "* Classification for mental health https://www.kaggle.com/datasets/reihanenamdari/mental-health-corpus\n",
        "* Depression identification https://www.kaggle.com/datasets/infamouscoder/depression-reddit-cleaned\n",
        "\n",
        "## Tutorials\n",
        "https://www.philschmid.de/instruction-tune-llama-2"
      ],
      "metadata": {
        "id": "bkTCPgfTLIZh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -q gradio\n",
        "!pip install -U -q datasets\n",
        "!pip install -U -q bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -U -q trl\n",
        "\n",
        "!pip install -U -q evaluate\n",
        "!pip install -U -q rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGXegQDzNtru",
        "outputId": "f66cd747-059b-4afe-e2a1-93d3260d3b18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "import yaml\n",
        "import gradio as gr\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import GenerationConfig, Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "import numpy as np\n",
        "from evaluate import load\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "TKvdMbEluPVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset instruction transformation"
      ],
      "metadata": {
        "id": "iiHYVFJyQuS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depression_dataset = load_dataset(\"vitaliy-sharandin/depression-instruct\")"
      ],
      "metadata": {
        "id": "LvA-W04yBvga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(examples):\n",
        "  output_texts = []\n",
        "  for i in range(len(examples['instruction'])):\n",
        "    if examples.get(\"context\", \"\") != \"\":\n",
        "        input_prompt = (f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n\"\n",
        "        f\"{examples['instruction'][i]}\\n\\n\"\n",
        "        f\"### Input: \\n\"\n",
        "        f\"{examples['context'][i]}\\n\\n\"\n",
        "        f\"### Response: \\n\"\n",
        "        f\"{examples['response'][i]}\")\n",
        "\n",
        "    else:\n",
        "      input_prompt = (f\"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n\"\n",
        "        f\"{examples['instruction'][i]}\\n\\n\"\n",
        "        f\"### Response:\\n\"\n",
        "        f\"{examples['response'][i]}\")\n",
        "\n",
        "    output_texts.append(input_prompt)\n",
        "\n",
        "  return output_texts\n",
        "\n",
        "def formatting_func_train(example):\n",
        "  if example.get(\"context\", \"\") != \"\":\n",
        "      input_prompt = (f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Input: \\n\"\n",
        "      f\"{example['context']}\\n\\n\"\n",
        "      f\"### Response: \\n\"\n",
        "      f\"{example['response']}\")\n",
        "\n",
        "  else:\n",
        "    input_prompt = (f\"Below is an instruction that describes a task. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Response:\\n\"\n",
        "      f\"{example['response']}\")\n",
        "\n",
        "  return {\"text\" : input_prompt}\n",
        "\n",
        "def formatting_func_eval(example):\n",
        "  if example.get(\"context\", \"\") != \"\":\n",
        "      input_prompt = (f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Input: \\n\"\n",
        "      f\"{example['context']}\\n\\n\")\n",
        "\n",
        "  else:\n",
        "    input_prompt = (f\"Below is an instruction that describes a task. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\")\n",
        "\n",
        "  return {\"text\" : input_prompt}"
      ],
      "metadata": {
        "id": "hwGaZ5y5A53T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_depression_dataset_train = depression_dataset.map(formatting_func_train)\n",
        "formatted_depression_dataset_eval = depression_dataset.map(formatting_func_eval)"
      ],
      "metadata": {
        "id": "riRBYQdGA7Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model load"
      ],
      "metadata": {
        "id": "-JWMiV2MM98j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Small model\n",
        "# model_name = \"EleutherAI/gpt-neo-125M\"\n",
        "\n",
        "# Big model\n",
        "# model_name = \"EleutherAI/gpt-neox-20b\"\n",
        "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "# model_name = \"NousResearch/Llama-2-7b-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "torch.manual_seed(42)\n",
        "print(base_model)"
      ],
      "metadata": {
        "id": "ysK6WLI_qs6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model instruction fine-tuning"
      ],
      "metadata": {
        "id": "C8z9BjCdYPuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning metrics definition"
      ],
      "metadata": {
        "id": "8k0etHPW4XmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bleu_rouge_f1(eval_pred):\n",
        "  predictions, labels, inputs = eval_pred\n",
        "  predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "  labels = [[idx for idx in label if idx != -100] for label in labels]\n",
        "  predictions = [[idx for idx in prediction if idx != -100] for prediction in predictions]\n",
        "  inputs = [[idx for idx in input if idx != -100] for input in inputs]\n",
        "\n",
        "  decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "  decoded_inputs = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "  for pred, label in zip(inputs, decoded_predictions, decoded_labels):\n",
        "    print(\"Inputs:\\n\\n\", inputs)\n",
        "    print(\"Prediction:\\n\\n\", pred)\n",
        "    print(\"Label:\\n\\n\", label)\n",
        "    print(\"----\")\n",
        "\n",
        "  print(f\"Prediction: {decoded_predictions}\\nLabel:{decoded_labels}\\n\\n\")\n",
        "\n",
        "  bleu = load(\"bleu\")\n",
        "  bleu_results = bleu.compute(predictions=decoded_predictions, references=decoded_labels)\n",
        "\n",
        "  rouge = load('rouge')\n",
        "  rouge_results = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n",
        "\n",
        "  f1 = 2 * (bleu_results['bleu'] * rouge_results['rouge1']) / (bleu_results['bleu'] + rouge_results['rouge1'])\n",
        "\n",
        "  scores = {\n",
        "        \"bleu\": bleu_results[\"bleu\"],\n",
        "        \"rouge1\": rouge_results[\"rouge1\"],\n",
        "        \"rouge2\": rouge_results[\"rouge2\"],\n",
        "        \"rougeL\": rouge_results[\"rougeL\"],\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "  return scores\n",
        "\n",
        "\n",
        "def fine_tune(model, train_dataset, eval_dataset, metric, only_evaluate=False):\n",
        "\n",
        "  qlora_config = LoraConfig(\n",
        "                            r=16,\n",
        "                            lora_alpha=32,\n",
        "                            lora_dropout=0.05,\n",
        "                            bias=\"none\",\n",
        "                            task_type=\"CAUSAL_LM\"\n",
        "                          )\n",
        "\n",
        "  model = prepare_model_for_kbit_training(model)\n",
        "  model = get_peft_model(model, qlora_config)\n",
        "  model.to('cuda')\n",
        "\n",
        "  supervised_finetuning_trainer = SFTTrainer(model,\n",
        "                                            train_dataset=train_dataset[\"train\"],\n",
        "                                            eval_dataset=eval_dataset[\"train\"],\n",
        "                                            args=TrainingArguments(\n",
        "                                                per_device_train_batch_size=4,\n",
        "                                                per_device_eval_batch_size=4,\n",
        "                                                gradient_accumulation_steps=4,\n",
        "                                                learning_rate=2e-4,\n",
        "                                                num_train_epochs=10,\n",
        "                                                output_dir=\"./wisai\",\n",
        "                                                optim=\"paged_adamw_8bit\",\n",
        "                                                fp16=True,\n",
        "                                            ),\n",
        "                                            # tokenizer=tokenizer,\n",
        "                                            peft_config=qlora_config,\n",
        "                                            # dataset_text_field=\"text\",\n",
        "                                            max_seq_length=2048,\n",
        "                                            # compute_metrics = metric,\n",
        "                                            data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "                                            formatting_func=formatting_func,\n",
        "                                            include_inputs_for_metrics = True\n",
        "                                            # data_collator=DataCollatorForCompletionOnlyLM(instruction_template='### Instruction:', response_template='### Response:', tokenizer=tokenizer, mlm=False)\n",
        "                                            # packing=True\n",
        "                                        )\n",
        "\n",
        "  # TRY PLAYING WITH COLLATOR???\n",
        "\n",
        "  # WRITE YOUR OWN IMPLEMENTATION OF EVALUATION\n",
        "    # predict all prompts from eval to list\n",
        "    # pass (predictions,correct_responses) to bleu_rouge_f1\n",
        "\n",
        "  eval_dataset_formatted = eval_dataset['train'].map(formatting_func_eval)\n",
        "  tokenized_eval_dataset = dataset.map(lambda examples: tokenizer(examples['text'], truncation=True), batched=True)\n",
        "\n",
        "  if only_evaluate:\n",
        "    predictions_output = supervised_finetuning_trainer.predict(tokenized_eval_dataset)\n",
        "  return\n",
        "\n",
        "  supervised_finetuning_trainer.train()\n",
        "  eval_result = supervised_finetuning_trainer.evaluate()\n",
        "\n",
        "  return eval_result"
      ],
      "metadata": {
        "id": "IsGjKseN3XFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Experiments\n",
        "1. Compare trained / untrained / small model results\n",
        "2. Complete training on all datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "5pGf2qH08XL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores_untrained_model = fine_tune(base_model, depression_dataset, depression_dataset, bleu_rouge_f1, only_evaluate=True)\n",
        "scores_untrained_model"
      ],
      "metadata": {
        "id": "9pcvtQXyPXwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_untrained_model = fine_tune(base_model, depression_dataset, depression_dataset, bleu_rouge_f1)\n",
        "scores_untrained_model"
      ],
      "metadata": {
        "id": "Gwei4jMTY5Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot lauch"
      ],
      "metadata": {
        "id": "Sc-PH7n8Y8un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gen_config = GenerationConfig(\n",
        "#     do_sample=True,\n",
        "#     temperature=0.9,\n",
        "#     max_new_tokens=150,\n",
        "#     pad_token_id=tokenizer.eos_token_id,\n",
        "#     num_return_sequences=1\n",
        "# )\n",
        "\n",
        "# def predict(prompt):\n",
        "#     encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "#     input_length = len(encoded_input[\"input_ids\"][0])\n",
        "#     output_ids = model.generate(generation_config=gen_config, **encoded_input)[0]\n",
        "#     output = tokenizer.decode(output_ids[input_length:], skip_special_tokens=True)\n",
        "#     return output\n",
        "\n",
        "# #gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\").launch()\n",
        "# print(predict(\"What is Depression?\"))"
      ],
      "metadata": {
        "id": "Fb_grdNpOarF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving model components to Huggingface"
      ],
      "metadata": {
        "id": "C9agpWsaZUL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# token = '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'\n",
        "# model.push_to_hub(\"wisai\", use_auth_token=token)\n",
        "# gen_config.push_to_hub(\"wisai\", \"generation_config.json\", use_auth_token=token)"
      ],
      "metadata": {
        "id": "i5vnd2GgwlkQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}