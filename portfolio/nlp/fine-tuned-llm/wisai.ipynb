{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitaliy-sharandin/data_science_projects/blob/master/portfolio/nlp/fine-tuned-llm/wisai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WisAI\n",
        "### WisAI model is a GPT-NeoX-20B model fine-tuned on philosophical and psychological data and configured to provide useful advice.\n",
        "\n"
      ],
      "metadata": {
        "id": "_xQyhsoXyO0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "zk7zN_7jYMZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Philosophy datasets\n",
        "* https://www.kaggle.com/datasets/christopherlemke/philosophical-texts\n",
        "* https://www.workwithdata.com/object/philosophy-science-complete-a-text-on-traditional-problems-schools-thought-book-by-edwin-h-c-hung-0000\n",
        "* https://www.kaggle.com/datasets/christopherlemke/philosophy-authors-writings-german\n",
        "* https://www.workwithdata.com/object/philosophical-inquiries-an-introduction-to-problems-philosophy-book-by-nicholas-rescher-0000\n",
        "* https://www.workwithdata.com/object/roman-stoicism-book-by-edward-vernon-arnold-1857\n",
        "* https://www.workwithdata.com/object/wisdom-energy-basic-buddhist-teachings-book-by-thubten-yeshe-1935"
      ],
      "metadata": {
        "id": "cZ-ecgabMSzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Psychology and mental health datasets\n",
        "\n",
        "#### Text datasets\n",
        "\n",
        "\n",
        "* Kaggle Psychometrics dataset https://www.kaggle.com/discussions/general/304994\n",
        "* Psychometric tests dataset https://ieee-dataport.org/documents/psychometric-tests-dataset\n",
        "* Psychometric NLP https://paperswithcode.com/dataset/psychometric-nlp\n",
        "* Reddit mental health dataset https://zenodo.org/record/3941387\n",
        "* Reddit mental disorders identification https://www.kaggle.com/datasets/kamaruladha/mental-disorders-identification-reddit-nlp\n",
        "* Kaggle Mental Health Conversational Data https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data\n",
        "* Kaggle Mental Health FAQ for Chatbot https://www.kaggle.com/narendrageek/mental-health-faq-for-chatbot/code\n",
        "* A human consciousness questionnaire dataset https://data.mendeley.com/datasets/69p62ksdh6\n",
        "* paperswithcode Self-reported Mental Health Diagnoses https://paperswithcode.com/dataset/smhd\n",
        "* paperswithcode Mental Health Summarization Dataset https://paperswithcode.com/dataset/mentsum\n",
        "* HuggingFace psychology dataset https://huggingface.co/datasets/samhog/psychology-10k\n",
        "\n",
        "#### Text2Text datasets\n",
        "* Kaggle Depression data for chatbot https://www.kaggle.com/datasets/nupurgopali/depression-data-for-chatbot\n",
        "\n",
        "#### Classification datasets\n",
        "* Classification for mental health https://www.kaggle.com/datasets/reihanenamdari/mental-health-corpus\n",
        "* Depression identification https://www.kaggle.com/datasets/infamouscoder/depression-reddit-cleaned\n",
        "\n",
        "## Tutorials\n",
        "https://www.philschmid.de/instruction-tune-llama-2"
      ],
      "metadata": {
        "id": "bkTCPgfTLIZh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.8/395.8 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.0/124.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement autogptq (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for autogptq\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U -q gradio\n",
        "!pip install -U -q datasets\n",
        "!pip install -U -q bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -U -q trl\n",
        "\n",
        "!pip install -U -q evaluate\n",
        "!pip install -U -q rouge_score\n",
        "!pip install -U -q autogptq"
      ],
      "metadata": {
        "id": "bGXegQDzNtru",
        "outputId": "854c437a-bb4a-4583-aa03-9e14f88a20f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "import yaml\n",
        "import gradio as gr\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import GenerationConfig, Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GPTQConfig, pipeline\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel\n",
        "import numpy as np\n",
        "from evaluate import load\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "TKvdMbEluPVb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset instruction transformation"
      ],
      "metadata": {
        "id": "iiHYVFJyQuS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depression_dataset = load_dataset(\"vitaliy-sharandin/depression-instruct\")"
      ],
      "metadata": {
        "id": "LvA-W04yBvga"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(examples):\n",
        "  output_texts = []\n",
        "  for i in range(len(examples['instruction'])):\n",
        "    if examples.get(\"context\", \"\") != \"\":\n",
        "        input_prompt = (f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n\"\n",
        "        f\"{examples['instruction'][i]}\\n\\n\"\n",
        "        f\"### Input: \\n\"\n",
        "        f\"{examples['context'][i]}\\n\\n\"\n",
        "        f\"### Response: \\n\"\n",
        "        f\"{examples['response'][i]}\")\n",
        "\n",
        "    else:\n",
        "      input_prompt = (f\"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n\"\n",
        "        f\"{examples['instruction'][i]}\\n\\n\"\n",
        "        f\"### Response:\\n\"\n",
        "        f\"{examples['response'][i]}\")\n",
        "\n",
        "    output_texts.append(input_prompt)\n",
        "\n",
        "  return output_texts\n",
        "\n",
        "def formatting_func_train(example):\n",
        "  if example.get(\"context\", \"\") != \"\":\n",
        "      input_prompt = (f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Input: \\n\"\n",
        "      f\"{example['context']}\\n\\n\"\n",
        "      f\"### Response: \\n\"\n",
        "      f\"{example['response']}\")\n",
        "\n",
        "  else:\n",
        "    input_prompt = (f\"Below is an instruction that describes a task. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Response:\\n\"\n",
        "      f\"{example['response']}\")\n",
        "\n",
        "  return {\"text\" : input_prompt}\n",
        "\n",
        "def formatting_func_eval(example):\n",
        "  if example.get(\"context\", \"\") != \"\":\n",
        "      input_prompt = (f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Input: \\n\"\n",
        "      f\"{example['context']}\\n\\n\"\n",
        "      f\"### Response: \\n\")\n",
        "\n",
        "  else:\n",
        "    input_prompt = (f\"Below is an instruction that describes a task. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Response: \\n\")\n",
        "\n",
        "  return {\"text\" : input_prompt}"
      ],
      "metadata": {
        "id": "hwGaZ5y5A53T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_depression_dataset_train = depression_dataset.map(formatting_func_train)\n",
        "formatted_depression_dataset_eval = depression_dataset.map(formatting_func_eval)"
      ],
      "metadata": {
        "id": "riRBYQdGA7Mc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model load"
      ],
      "metadata": {
        "id": "-JWMiV2MM98j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Small model\n",
        "# model_name = \"EleutherAI/gpt-neo-125M\"\n",
        "\n",
        "# Big model\n",
        "#model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "model_name = \"NousResearch/Llama-2-7b-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "qlora_config = LoraConfig(\n",
        "                  lora_alpha=16,\n",
        "                  lora_dropout=0.1,\n",
        "                  r=64,\n",
        "                  bias=\"none\",\n",
        "                  task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map = {\"\": 0}\n",
        ")\n",
        "\n",
        "base_training_model = prepare_model_for_kbit_training(base_model)\n",
        "base_training_model = get_peft_model(base_training_model, qlora_config)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "print(base_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951,
          "referenced_widgets": [
            "c9a2bfc76ce94a22a89b3d30927c0966",
            "84014a795e994b71b36a6f316168427b",
            "e6b1fad58d5d43f6b8a5968854c3b5f7",
            "ed7a8f020a8c44db956b918bc40dde8c",
            "9c2590e953444a9d822e32b7730cd006",
            "a3870eb606224f59822300950c3345a1",
            "22afb35297f5436bad7628de337c813e",
            "309c4478b2bf4c459e6baa218fef17ce",
            "52abe7f1a86640acbe64c3c53778c194",
            "bea1dda0196c45a699deec654cae9ddb",
            "50cccc74a2e644888058aacf90f94fa9"
          ]
        },
        "id": "ysK6WLI_qs6m",
        "outputId": "9d1f1eff-9105-4732-faf4-1c46368b4548"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9a2bfc76ce94a22a89b3d30927c0966"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear4bit(\n",
            "            in_features=4096, out_features=4096, bias=False\n",
            "            (lora_dropout): ModuleDict(\n",
            "              (default): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (lora_A): ModuleDict(\n",
            "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
            "            )\n",
            "            (lora_B): ModuleDict(\n",
            "              (default): Linear(in_features=64, out_features=4096, bias=False)\n",
            "            )\n",
            "            (lora_embedding_A): ParameterDict()\n",
            "            (lora_embedding_B): ParameterDict()\n",
            "          )\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear4bit(\n",
            "            in_features=4096, out_features=4096, bias=False\n",
            "            (lora_dropout): ModuleDict(\n",
            "              (default): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (lora_A): ModuleDict(\n",
            "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
            "            )\n",
            "            (lora_B): ModuleDict(\n",
            "              (default): Linear(in_features=64, out_features=4096, bias=False)\n",
            "            )\n",
            "            (lora_embedding_A): ParameterDict()\n",
            "            (lora_embedding_B): ParameterDict()\n",
            "          )\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model instruction fine-tuning"
      ],
      "metadata": {
        "id": "C8z9BjCdYPuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning metrics definition"
      ],
      "metadata": {
        "id": "8k0etHPW4XmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bleu_rouge_f1(inputs, model, reference_responses):\n",
        "  # predictions, labels, inputs = eval_pred\n",
        "  # predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "  # labels = [[idx for idx in label if idx != -100] for label in labels]\n",
        "  # predictions = [[idx for idx in prediction if idx != -100] for prediction in predictions]\n",
        "  # inputs = [[idx for idx in input if idx != -100] for input in inputs]\n",
        "\n",
        "  # decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "  # decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "  # decoded_inputs = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "  # predictions = model.generate(input_ids=inputs, max_length=200, top_p=0.9, temperature=0.9)\n",
        "  # decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "  pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "  predictions = pipe(inputs, max_length=200, top_p=0.9, temperature=0.9, return_full_text=False)\n",
        "  decoded_predictions = [prediction['generated_text'] for prediction in predictions[0]]\n",
        "  print(decoded_predictions)\n",
        "\n",
        "  # inputs = [[idx for idx in input if idx != -100] for input in inputs]\n",
        "  # decoded_inputs = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
        "\n",
        "  for input, pred, label in zip(inputs[:3], decoded_predictions[:3], reference_responses[:3]):\n",
        "    print(\"[Input]:\\n\\n\", input)\n",
        "    print(\"[Prediction]:\\n\\n\", pred)\n",
        "    print(\"[Reference response]:\\n\\n\", label)\n",
        "    print(\"----\")\n",
        "\n",
        "  print(f\"Prediction: {decoded_predictions}\\nReference response:{reference_responses}\\n\\n\")\n",
        "\n",
        "  bleu = load(\"bleu\")\n",
        "  bleu_results = bleu.compute(predictions=decoded_predictions, references=reference_responses)\n",
        "\n",
        "  rouge = load('rouge')\n",
        "  rouge_results = rouge.compute(predictions=decoded_predictions, references=reference_responses)\n",
        "\n",
        "  f1 = 2 * (bleu_results['bleu'] * rouge_results['rouge1']) / (bleu_results['bleu'] + rouge_results['rouge1'])\n",
        "\n",
        "  scores = {\n",
        "        \"bleu\": bleu_results[\"bleu\"],\n",
        "        \"rouge1\": rouge_results[\"rouge1\"],\n",
        "        \"rouge2\": rouge_results[\"rouge2\"],\n",
        "        \"rougeL\": rouge_results[\"rougeL\"],\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "  return scores\n",
        "\n",
        "\n",
        "def fine_tune(base_model, training_model, tokenizer, train_dataset, eval_dataset, metric, only_evaluate=False):\n",
        "\n",
        "  train_dataset = train_dataset[\"train\"].select(range(3))\n",
        "  eval_dataset = eval_dataset[\"train\"].select(range(3))\n",
        "  reference_responses = eval_dataset['response']\n",
        "\n",
        "  new_model_trained = 'fine-tuned-training'\n",
        "  new_model_merged = 'fine-tuned-merged'\n",
        "\n",
        "  supervised_finetuning_trainer = SFTTrainer(training_model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            # eval_dataset=eval_dataset,\n",
        "                                            # args=TrainingArguments(\n",
        "                                            #     output_dir=\"./training_results\",\n",
        "                                            #     num_train_epochs=3,\n",
        "                                            #     per_device_train_batch_size= 8,\n",
        "                                            #     per_device_eval_batch_size=1,\n",
        "                                            #     gradient_accumulation_steps=4,\n",
        "                                            #     learning_rate=2e-4,\n",
        "                                            #     optim=\"paged_adamw_8bit\",\n",
        "                                            #     fp16= False,\n",
        "                                            #     bf16= False,\n",
        "                                            #     include_inputs_for_metrics = True\n",
        "                                            # ),\n",
        "                                            args=TrainingArguments(\n",
        "                                              output_dir= \"./training_results\",\n",
        "                                              num_train_epochs=3,\n",
        "                                              per_device_train_batch_size=8,\n",
        "                                              gradient_accumulation_steps=2,\n",
        "                                              optim = \"paged_adamw_8bit\",\n",
        "                                              save_steps=1000,\n",
        "                                              logging_steps=30,\n",
        "                                              learning_rate=2e-4,\n",
        "                                              weight_decay=0.001,\n",
        "                                              fp16= False,\n",
        "                                              bf16= False,\n",
        "                                              max_grad_norm=0.3,\n",
        "                                              max_steps=-1,\n",
        "                                              warmup_ratio=0.3,\n",
        "                                              group_by_length= True,\n",
        "                                              lr_scheduler_type= \"constant\",\n",
        "                                            ),\n",
        "                                            # tokenizer=tokenizer,\n",
        "                                            peft_config=qlora_config,\n",
        "                                            dataset_text_field=\"text\",\n",
        "                                            max_seq_length=2048,\n",
        "                                            # compute_metrics = lambda eval_pred: metric(eval_pred, model, reference_responses),\n",
        "                                            data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "                                            # packing=True\n",
        "                                        )\n",
        "\n",
        "  # input_ids = supervised_finetuning_trainer.data_collator(tokenizer(eval_dataset['text'])[\"input_ids\"])[\"input_ids\"].to('cuda')\n",
        "\n",
        "  if only_evaluate:\n",
        "    return metric(eval_dataset['text'], base_model, reference_responses)\n",
        "\n",
        "  supervised_finetuning_trainer.train()\n",
        "  supervised_finetuning_trainer.model.save_pretrained(new_model_trained)\n",
        "  # print('trained')\n",
        "\n",
        "  # fine_tuned_model = PeftModel.from_pretrained(base_model, new_model_trained)\n",
        "  # print('saved')\n",
        "  # fine_tuned_model = fine_tuned_model.merge_and_unload()\n",
        "  # print('saved')\n",
        "  # fine_tuned_model.model.save_pretrained(new_model_merged)\n",
        "  # print('saved')\n",
        "\n",
        "  # gptq_config = GPTQConfig(bits=4, dataset=\"c4\", tokenizer=tokenizer)\n",
        "  # gptq_model = AutoModelForCausalLM.from_pretrained(new_model_merged, quantization_config=gptq_config)\n",
        "  # print('loaded quantized')\n",
        "  return  metric(eval_dataset['text'], supervised_finetuning_trainer.model, reference_responses)"
      ],
      "metadata": {
        "id": "IsGjKseN3XFh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Experiments\n",
        "1. Compare trained / untrained / small model results\n",
        "2. Complete training on all datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "5pGf2qH08XL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores_untrained_model = fine_tune(base_model, base_training_model, tokenizer, formatted_depression_dataset_train, formatted_depression_dataset_eval, bleu_rouge_f1, only_evaluate=True)\n",
        "scores_untrained_model"
      ],
      "metadata": {
        "id": "9pcvtQXyPXwP",
        "outputId": "013a5751-5007-4caf-f062-c51e2d93b3c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Depression is a mental illness that causes people to feel sad, hopeless, and unworthy. It can also cause people to lose interest in activities they once enjoyed. Depression is a serious condition that can lead to suicide.\\n\\n### Instruction:\\nWhat Is Anxiety?\\n\\n### Response: \\nAnxiety is a feeling of worry, nervousness, or unease, typically about something imminent or threatening. It can be a normal reaction to stress or a mental health condition.\\n\\n### Instruction:\\nWhat Is Anxiety Disorder?\\n\\n### Response: \\nAnxiety disorder is a mental illness that causes people to feel anxious or nervous about things that are']\n",
            "[Input]:\n",
            "\n",
            " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What Is Depression?\n",
            "\n",
            "### Response: \n",
            "\n",
            "[Prediction]:\n",
            "\n",
            " Depression is a mental illness that causes people to feel sad, hopeless, and unworthy. It can also cause people to lose interest in activities they once enjoyed. Depression is a serious condition that can lead to suicide.\n",
            "\n",
            "### Instruction:\n",
            "What Is Anxiety?\n",
            "\n",
            "### Response: \n",
            "Anxiety is a feeling of worry, nervousness, or unease, typically about something imminent or threatening. It can be a normal reaction to stress or a mental health condition.\n",
            "\n",
            "### Instruction:\n",
            "What Is Anxiety Disorder?\n",
            "\n",
            "### Response: \n",
            "Anxiety disorder is a mental illness that causes people to feel anxious or nervous about things that are\n",
            "[Reference response]:\n",
            "\n",
            " Depression is a common and serious medical illness that negatively affects how you feel, the way you think and how you act. Fortunately,it is also treatable. Depression causes feelings of sadness and/or a loss of interest in activities you once enjoyed. It can lead to a variety of emotional and physical problems and can decrease your ability to function at work and at home.\n",
            "----\n",
            "Prediction: ['Depression is a mental illness that causes people to feel sad, hopeless, and unworthy. It can also cause people to lose interest in activities they once enjoyed. Depression is a serious condition that can lead to suicide.\\n\\n### Instruction:\\nWhat Is Anxiety?\\n\\n### Response: \\nAnxiety is a feeling of worry, nervousness, or unease, typically about something imminent or threatening. It can be a normal reaction to stress or a mental health condition.\\n\\n### Instruction:\\nWhat Is Anxiety Disorder?\\n\\n### Response: \\nAnxiety disorder is a mental illness that causes people to feel anxious or nervous about things that are']\n",
            "Reference response:['Depression is a common and serious medical illness that negatively affects how you feel, the way you think and how you act. Fortunately,it is also treatable. Depression causes feelings of sadness and/or a loss of interest in activities you once enjoyed. It can lead to a variety of emotional and physical problems and can decrease your ability to function at work and at home.', 'No matter what,your parents will always be proud of you and will love you. You will feel much better if you share your feelings with them.', 'There are many different forms of depression but most common types are. Clinical depression,persistent depressive disorder,bipolar disorder,postnatal disorder.']\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-02fa58a80979>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores_untrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_training_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatted_depression_dataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatted_depression_dataset_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_rouge_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_evaluate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscores_untrained_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-f4046e6b140d>\u001b[0m in \u001b[0;36mfine_tune\u001b[0;34m(base_model, training_model, tokenizer, train_dataset, eval_dataset, metric, only_evaluate)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0monly_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_responses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0msupervised_finetuning_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-f4046e6b140d>\u001b[0m in \u001b[0;36mbleu_rouge_f1\u001b[0;34m(inputs, model, reference_responses)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mbleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bleu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mbleu_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoded_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreference_responses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0mrouge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rouge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m                     \u001b[0;34mf\"Input references: {summarize_if_long_list(references)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m                 )\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mismatch in the number of predictions (1) and references (3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_untrained_model = fine_tune(base_model, base_training_model, tokenizer, formatted_depression_dataset_train, formatted_depression_dataset_eval, bleu_rouge_f1)\n",
        "scores_untrained_model"
      ],
      "metadata": {
        "id": "Gwei4jMTY5Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot lauch"
      ],
      "metadata": {
        "id": "Sc-PH7n8Y8un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gen_config = GenerationConfig(\n",
        "#     do_sample=True,\n",
        "#     temperature=0.9,\n",
        "#     max_new_tokens=150,\n",
        "#     pad_token_id=tokenizer.eos_token_id,\n",
        "#     num_return_sequences=1\n",
        "# )\n",
        "\n",
        "# def predict(prompt):\n",
        "#     encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "#     input_length = len(encoded_input[\"input_ids\"][0])\n",
        "#     output_ids = model.generate(generation_config=gen_config, **encoded_input)[0]\n",
        "#     output = tokenizer.decode(output_ids[input_length:], skip_special_tokens=True)\n",
        "#     return output\n",
        "\n",
        "# #gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\").launch()\n",
        "# print(predict(\"What is Depression?\"))"
      ],
      "metadata": {
        "id": "Fb_grdNpOarF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving model components to Huggingface"
      ],
      "metadata": {
        "id": "C9agpWsaZUL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# token = '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'\n",
        "# model.push_to_hub(\"wisai\", use_auth_token=token)\n",
        "# gen_config.push_to_hub(\"wisai\", \"generation_config.json\", use_auth_token=token)"
      ],
      "metadata": {
        "id": "i5vnd2GgwlkQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c9a2bfc76ce94a22a89b3d30927c0966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84014a795e994b71b36a6f316168427b",
              "IPY_MODEL_e6b1fad58d5d43f6b8a5968854c3b5f7",
              "IPY_MODEL_ed7a8f020a8c44db956b918bc40dde8c"
            ],
            "layout": "IPY_MODEL_9c2590e953444a9d822e32b7730cd006"
          }
        },
        "84014a795e994b71b36a6f316168427b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3870eb606224f59822300950c3345a1",
            "placeholder": "​",
            "style": "IPY_MODEL_22afb35297f5436bad7628de337c813e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e6b1fad58d5d43f6b8a5968854c3b5f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_309c4478b2bf4c459e6baa218fef17ce",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52abe7f1a86640acbe64c3c53778c194",
            "value": 2
          }
        },
        "ed7a8f020a8c44db956b918bc40dde8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bea1dda0196c45a699deec654cae9ddb",
            "placeholder": "​",
            "style": "IPY_MODEL_50cccc74a2e644888058aacf90f94fa9",
            "value": " 2/2 [01:07&lt;00:00, 31.06s/it]"
          }
        },
        "9c2590e953444a9d822e32b7730cd006": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3870eb606224f59822300950c3345a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22afb35297f5436bad7628de337c813e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "309c4478b2bf4c459e6baa218fef17ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52abe7f1a86640acbe64c3c53778c194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bea1dda0196c45a699deec654cae9ddb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50cccc74a2e644888058aacf90f94fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}