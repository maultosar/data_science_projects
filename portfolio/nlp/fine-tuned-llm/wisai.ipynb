{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitaliy-sharandin/data_science_projects/blob/master/portfolio/nlp/fine-tuned-llm/wisai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WisAI\n",
        "### WisAI model is a GPT-NeoX-20B model fine-tuned on philosophical and psychological data and configured to provide useful advice."
      ],
      "metadata": {
        "id": "_xQyhsoXyO0K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "source": [
        "!pip install -U -q gradio\n",
        "!pip install -U -q transformers\n",
        "!pip install -U -q datasets\n",
        "!pip install -U -q accelerate\n",
        "!pip install -U -q bitsandbytes\n",
        "!pip install -U -q peft\n",
        "!pip install -U -q trl\n",
        "\n",
        "!pip install -U -q evaluate\n",
        "!pip install -U -q rouge_score"
      ],
      "metadata": {
        "id": "bGXegQDzNtru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "import yaml\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import GenerationConfig, Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig\n",
        "import numpy as np\n",
        "from evaluate import load"
      ],
      "metadata": {
        "id": "TKvdMbEluPVb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Small model\n",
        "small_model_name = \"EleutherAI/gpt-neo-125M\"\n",
        "\n",
        "small_tokenizer = AutoTokenizer.from_pretrained(small_model_name)\n",
        "small_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# small_model = AutoModelForCausalLM.from_pretrained(small_model_name)\n",
        "\n",
        "\n",
        "\n",
        "# Base model\n",
        "model_name = \"EleutherAI/gpt-neox-20b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\":0}\n",
        ")\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(main_model_name)\n",
        "# # model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "torch.manual_seed(42)\n",
        "print(base_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "ysK6WLI_qs6m",
        "outputId": "4d67f01e-eeef-4626-8f08-31c28470c7f7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/IPython/core/\u001b[0m\u001b[1;33minteractiveshell.py\u001b[0m:\u001b[94m3553\u001b[0m in \u001b[92mrun_code\u001b[0m        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3550 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melif\u001b[0m async_ :                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3551 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mawait\u001b[0m \u001b[96meval\u001b[0m(code_obj, \u001b[96mself\u001b[0m.user_global_ns, \u001b[96mself\u001b[0m.user_ns)               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3552 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3553 \u001b[2m│   │   │   │   │   \u001b[0mexec(code_obj, \u001b[96mself\u001b[0m.user_global_ns, \u001b[96mself\u001b[0m.user_ns)                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3554 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfinally\u001b[0m:                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3555 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Reset our crash handler in place\u001b[0m                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3556 \u001b[0m\u001b[2m│   │   │   │   \u001b[0msys.excepthook = old_excepthook                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 24>\u001b[0m:\u001b[94m24\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mauto_factory.py\u001b[0m:\u001b[94m484\u001b[0m in          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92mfrom_pretrained\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m481 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m482 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mtype\u001b[0m(config) \u001b[95min\u001b[0m \u001b[96mcls\u001b[0m._model_mapping.keys():                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m483 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_class = _get_model_class(config, \u001b[96mcls\u001b[0m._model_mapping)                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m484 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m model_class.from_pretrained(                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m487 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m2257\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2254 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2255 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m load_in_8bit \u001b[95mor\u001b[0m load_in_4bit:                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2256 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (is_accelerate_available() \u001b[95mand\u001b[0m is_bitsandbytes_available()):           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2257 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mImportError\u001b[0m(                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2258 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mUsing `load_in_8bit=True` requires Accelerate: `pip install acceler\u001b[0m  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2259 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsand\u001b[0m  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2260 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m pip install bitsandbytes` \u001b[0m\u001b[33m\"\u001b[0m                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mImportError: \u001b[0mUsing `\u001b[33mload_in_8bit\u001b[0m=\u001b[3;92mTrue\u001b[0m` requires Accelerate: `pip install accelerate` and the latest version of \n",
              "bitsandbytes `pip install -i \u001b[4;94mhttps://test.pypi.org/simple/\u001b[0m bitsandbytes` or pip install bitsandbytes` \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/IPython/core/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">interactiveshell.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3553</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">run_code</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3550 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> async_ :                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3551 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">await</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">eval</span>(code_obj, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_global_ns, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_ns)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3552 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3553 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>exec(code_obj, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_global_ns, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_ns)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3554 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3555 │   │   │   │   # Reset our crash handler in place</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3556 │   │   │   │   </span>sys.excepthook = old_excepthook                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 24&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">24</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/auto/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">auto_factory.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">484</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">481 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">482 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(config) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>._model_mapping.keys():                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">483 │   │   │   </span>model_class = _get_model_class(config, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>._model_mapping)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>484 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> model_class.from_pretrained(                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">485 │   │   │   │   </span>pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">486 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">487 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2257</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2254 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2255 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> load_in_8bit <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> load_in_4bit:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2256 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (is_accelerate_available() <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> is_bitsandbytes_available()):           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2257 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ImportError</span>(                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2258 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Using `load_in_8bit=True` requires Accelerate: `pip install acceler</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2259 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\" bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsand</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2260 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\" pip install bitsandbytes` \"</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ImportError: </span>Using `<span style=\"color: #808000; text-decoration-color: #808000\">load_in_8bit</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>` requires Accelerate: `pip install accelerate` and the latest version of \n",
              "bitsandbytes `pip install -i <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://test.pypi.org/simple/</span> bitsandbytes` or pip install bitsandbytes` \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "XfUuLj5uzIzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training datasets list"
      ],
      "metadata": {
        "id": "zk7zN_7jYMZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Psychology and mental health datasets\n",
        "\n",
        "#### Text datasets\n",
        "\n",
        "\n",
        "* Kaggle Psychometrics dataset https://www.kaggle.com/discussions/general/304994\n",
        "* Psychometric tests dataset https://ieee-dataport.org/documents/psychometric-tests-dataset\n",
        "* Psychometric NLP https://paperswithcode.com/dataset/psychometric-nlp\n",
        "* Reddit mental health dataset https://zenodo.org/record/3941387\n",
        "* Reddit mental disorders identification https://www.kaggle.com/datasets/kamaruladha/mental-disorders-identification-reddit-nlp\n",
        "* Kaggle Mental Health Conversational Data https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data\n",
        "* Kaggle Mental Health FAQ for Chatbot https://www.kaggle.com/narendrageek/mental-health-faq-for-chatbot/code\n",
        "* A human consciousness questionnaire dataset https://data.mendeley.com/datasets/69p62ksdh6\n",
        "* paperswithcode Self-reported Mental Health Diagnoses https://paperswithcode.com/dataset/smhd\n",
        "* paperswithcode Mental Health Summarization Dataset https://paperswithcode.com/dataset/mentsum\n",
        "* HuggingFace psychology dataset https://huggingface.co/datasets/samhog/psychology-10k\n",
        "\n",
        "#### Text2Text datasets\n",
        "* Kaggle Depression data for chatbot https://www.kaggle.com/datasets/nupurgopali/depression-data-for-chatbot\n",
        "\n",
        "#### Classification datasets\n",
        "* Classification for mental health https://www.kaggle.com/datasets/reihanenamdari/mental-health-corpus\n",
        "* Depression identification https://www.kaggle.com/datasets/infamouscoder/depression-reddit-cleaned"
      ],
      "metadata": {
        "id": "bkTCPgfTLIZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Philosophy datasets\n",
        "* https://www.kaggle.com/datasets/christopherlemke/philosophical-texts\n",
        "* https://www.workwithdata.com/object/philosophy-science-complete-a-text-on-traditional-problems-schools-thought-book-by-edwin-h-c-hung-0000\n",
        "* https://www.kaggle.com/datasets/christopherlemke/philosophy-authors-writings-german\n",
        "* https://www.workwithdata.com/object/philosophical-inquiries-an-introduction-to-problems-philosophy-book-by-nicholas-rescher-0000\n",
        "* https://www.workwithdata.com/object/roman-stoicism-book-by-edward-vernon-arnold-1857\n",
        "* https://www.workwithdata.com/object/wisdom-energy-basic-buddhist-teachings-book-by-thubten-yeshe-1935"
      ],
      "metadata": {
        "id": "cZ-ecgabMSzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training dataset creation"
      ],
      "metadata": {
        "id": "Zx3WUvwZQA3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data load and utility methods"
      ],
      "metadata": {
        "id": "iiHYVFJyQuS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "depression_data = []\n",
        "\n",
        "with open('/content/drive/MyDrive/Data/depression.yml', 'r') as file:\n",
        "     depression_data = yaml.safe_load(file)"
      ],
      "metadata": {
        "id": "o0lcHxuiFcC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_depression_dataset(conversations):\n",
        "  output = {'instruction':[],'response':[]}\n",
        "  for convo in conversations:\n",
        "    completion = ''\n",
        "    for i, dialog in enumerate(convo):\n",
        "      if i == 0:\n",
        "        prompt = dialog\n",
        "        # p_encode = prompt.encode(\"ascii\", \"ignore\")\n",
        "        # prompt = p_encode.decode()\n",
        "        prompt = prompt.replace(\"\\xa0\", \" \")\n",
        "        # print('prompt:',prompt)\n",
        "      else:\n",
        "        completion += \" \" + dialog\n",
        "        # c_encode = completion.encode(\"ascii\", \"ignore\")\n",
        "        # completion = c_encode.decode()\n",
        "        completion = completion.replace(\"\\xa0\", \" \")\n",
        "    completion = completion.strip()\n",
        "    # print(line)\n",
        "    output['instruction'].append(prompt)\n",
        "    output['response'].append(completion)\n",
        "  return output"
      ],
      "metadata": {
        "id": "p-oe6v9RH0pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(example):\n",
        "  if example.get(\"context\", \"\") != \"\":\n",
        "      input_prompt = (f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Input: \\n\"\n",
        "      f\"{example['context']}\\n\\n\"\n",
        "      f\"### Response: \\n\"\n",
        "      f\"{example['response']}\")\n",
        "\n",
        "  else:\n",
        "    input_prompt = (f\"Below is an instruction that describes a task. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      \"### Instruction:\\n\"\n",
        "      f\"{example['instruction']}\\n\\n\"\n",
        "      f\"### Response:\\n\"\n",
        "      f\"{example['response']}\")\n",
        "\n",
        "  return {\"text\" : input_prompt}"
      ],
      "metadata": {
        "id": "hwGaZ5y5A53T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_depression_data = parse_depression_dataset(depression_data['conversations'])\n",
        "# depression_df = pd.DataFrame(parsed_depression_data)\n",
        "depression_dataset = Dataset.from_dict(parsed_depression_data).train_test_split(test_size=0.1)\n",
        "formatted_depression_dataset = depression_dataset.map(formatting_func)"
      ],
      "metadata": {
        "id": "riRBYQdGA7Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training phase"
      ],
      "metadata": {
        "id": "C8z9BjCdYPuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training utility methods"
      ],
      "metadata": {
        "id": "8k0etHPW4XmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bleu_rouge_f1(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "  labels = [[idx for idx in label if idx != -100] for label in labels]\n",
        "\n",
        "  decoded_predictions = [tokenizer.decode(pred) for pred in predictions]\n",
        "  decoded_labels = [tokenizer.decode(label) for label in labels]\n",
        "\n",
        "  # print(f\"Prediction: {decoded_predictions}\\nLabel:{decoded_labels}\\n\")\n",
        "\n",
        "  bleu = load(\"bleu\")\n",
        "  bleu_results = bleu.compute(predictions=decoded_predictions, references=decoded_labels)\n",
        "\n",
        "  rouge = load('rouge')\n",
        "  rouge_results = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n",
        "\n",
        "  f1 = 2 * (bleu_results['bleu'] * rouge_results['rouge1']) / (bleu_results['bleu'] + rouge_results['rouge1'])\n",
        "\n",
        "  scores = {\n",
        "        \"bleu\": bleu_results[\"bleu\"],\n",
        "        \"rouge1\": rouge_results[\"rouge1\"],\n",
        "        \"rouge2\": rouge_results[\"rouge2\"],\n",
        "        \"rougeL\": rouge_results[\"rougeL\"],\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "  return scores\n",
        "\n",
        "\n",
        "def train_model(model, formatted_dataset, metric):\n",
        "\n",
        "  qlora_config = LoraConfig(\n",
        "                            r=16,\n",
        "                            lora_alpha=32,\n",
        "                            lora_dropout=0.05,\n",
        "                            bias=\"none\",\n",
        "                            task_type=\"CAUSAL_LM\"\n",
        "                          )\n",
        "\n",
        "  supervised_finetuning_trainer = SFTTrainer(base_model,\n",
        "                                            train_dataset=formatted_dataset[\"train\"],\n",
        "                                            eval_dataset=formatted_dataset[\"test\"],\n",
        "                                            args=transformers.TrainingArguments(\n",
        "                                                per_device_train_batch_size=1,\n",
        "                                                gradient_accumulation_steps=4,\n",
        "                                                learning_rate=2e-4,\n",
        "                                                max_steps=5000,\n",
        "                                                output_dir=\"./wisai\",\n",
        "                                                optim=\"paged_adamw_8bit\",\n",
        "                                                fp16=True,\n",
        "                                            ),\n",
        "                                            tokenizer=tokenizer,\n",
        "                                            peft_config=qlora_config,\n",
        "                                            dataset_text_field=\"text\",\n",
        "                                            max_seq_length=512\n",
        "                                        )\n",
        "\n",
        "  supervised_finetuning_trainer.train()\n",
        "\n",
        "  eval_result = supervised_finetuning_trainer.evaluate()\n",
        "\n",
        "  return eval_result\n",
        "\n",
        "\n",
        "def pretraining_prediction_scores(model, tokenized_dataset, data_collator, metric):\n",
        "\n",
        "  qlora_config = LoraConfig(\n",
        "                            r=16,\n",
        "                            lora_alpha=32,\n",
        "                            lora_dropout=0.05,\n",
        "                            bias=\"none\",\n",
        "                            task_type=\"CAUSAL_LM\"\n",
        "                          )\n",
        "\n",
        "  supervised_finetuning_trainer = SFTTrainer(base_model,\n",
        "                                            train_dataset=formatted_dataset[\"train\"],\n",
        "                                            eval_dataset=formatted_dataset[\"test\"],\n",
        "                                            args=transformers.TrainingArguments(\n",
        "                                                per_device_train_batch_size=1,\n",
        "                                                gradient_accumulation_steps=4,\n",
        "                                                learning_rate=2e-4,\n",
        "                                                max_steps=5000,\n",
        "                                                output_dir=\"./wisai\",\n",
        "                                                optim=\"paged_adamw_8bit\",\n",
        "                                                fp16=True,\n",
        "                                            ),\n",
        "                                            tokenizer=tokenizer,\n",
        "                                            peft_config=qlora_config,\n",
        "                                            dataset_text_field=\"text\",\n",
        "                                            max_seq_length=512,\n",
        "                                            compute_metrics = metric\n",
        "                                        )\n",
        "\n",
        "  eval_result = supervised_finetuning_trainer.evaluate()\n",
        "\n",
        "  return eval_result"
      ],
      "metadata": {
        "id": "IsGjKseN3XFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "Z8f90dC64RwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Experiments\n",
        "1. Compare trained / untrained / small model results\n",
        "2. Complete training on all datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "5pGf2qH08XL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scores_pretrained_model = pretraining_prediction_scores(reference_base_model, tokenized_dataset, data_collator_seq2seq, bleu_rouge_f1)\n",
        "# scores_pretrained_model"
      ],
      "metadata": {
        "id": "9pcvtQXyPXwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = train_model(base_model, tokenized_dataset, data_collator_seq2seq, bleu_rouge_f1)\n",
        "scores"
      ],
      "metadata": {
        "id": "Gwei4jMTY5Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot lauch"
      ],
      "metadata": {
        "id": "Sc-PH7n8Y8un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_config = GenerationConfig(\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    max_new_tokens=150,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "def predict(prompt):\n",
        "    encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "    input_length = len(encoded_input[\"input_ids\"][0])\n",
        "    output_ids = model.generate(generation_config=gen_config, **encoded_input)[0]\n",
        "    output = tokenizer.decode(output_ids[input_length:], skip_special_tokens=True)\n",
        "    return output\n",
        "\n",
        "#gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\").launch()\n",
        "print(predict(\"What is Depression?\"))"
      ],
      "metadata": {
        "id": "Fb_grdNpOarF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving model components to Huggingface"
      ],
      "metadata": {
        "id": "C9agpWsaZUL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# token = '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'\n",
        "model.push_to_hub(\"wisai\", use_auth_token=token)\n",
        "# gen_config.push_to_hub(\"wisai\", \"generation_config.json\", use_auth_token=token)"
      ],
      "metadata": {
        "id": "i5vnd2GgwlkQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}