{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitaliy-sharandin/data_science_projects/blob/master/portfolio/regression/Energy_price_demand_and_potential_supply.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Energy demand prediction of Spain's major cities\n",
        "\n",
        "This project uses energy demand information from major Spanish cities to predict cumulative energy consumption."
      ],
      "metadata": {
        "id": "5g5sn0fed2SV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "Spanish major cities energy consumption and weather dataset\n",
        "* https://www.kaggle.com/datasets/nicholasjhana/energy-consumption-generation-prices-and-weather\n",
        "\n"
      ],
      "metadata": {
        "id": "vrrnzKooh7uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q datasets\n",
        "!pip install -U -q ydata-profiling\n",
        "!pip install -U -q feature_engine\n",
        "!pip install -U -q Boruta\n",
        "!pip install -U -q optuna\n",
        "!pip install -U -q eli5\n",
        "!pip install -U -q statsforecast\n",
        "!pip install -U -q mlforecast\n",
        "!pip install -U -q neuralforecast\n",
        "!pip install -U -q datasetsforecast"
      ],
      "metadata": {
        "id": "cipdnuo5bq8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e702bf3-b573-4f52-9fab-d2e465e33bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.5/357.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.7/102.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.5/679.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.9/328.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA\n"
      ],
      "metadata": {
        "id": "AemoxCzzBtDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from ydata_profiling import ProfileReport\n",
        "import pandas as pd\n",
        "from feature_engine.encoding import OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from boruta import BorutaPy\n",
        "import xgboost as xgb\n",
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from datasetsforecast.losses import rmse\n",
        "from statsforecast import StatsForecast\n",
        "from statsforecast.models import AutoARIMA\n",
        "from mlforecast import MLForecast\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from neuralforecast import NeuralForecast\n",
        "from neuralforecast.models import LSTM\n",
        "from neuralforecast.losses.pytorch import DistributionLoss"
      ],
      "metadata": {
        "id": "VR-5zkUkNZk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlNrYf_87VRa"
      },
      "outputs": [],
      "source": [
        "energy_consumption_dataset = load_dataset(\"vitaliy-sharandin/energy-consumption-hourly-spain\")\n",
        "energy_consumption_weather_dataset = load_dataset(\"vitaliy-sharandin/energy-consumption-weather-hourly-spain\")\n",
        "energy_df = energy_consumption_dataset['train'].to_pandas()\n",
        "weather_df = energy_consumption_weather_dataset['train'].to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Energy dataset"
      ],
      "metadata": {
        "id": "ZKvWZvBF-NPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "energy_df['time'] = pd.to_datetime(energy_df['time'], errors=\"coerce\", utc=True)\n",
        "energy_df = energy_df.set_index('time')\n",
        "energy_df.index = pd.to_datetime(energy_df.index,utc=True)\n",
        "energy_df=energy_df.asfreq('h')"
      ],
      "metadata": {
        "id": "X_BFfmUWhtcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# profile = ProfileReport(energy_df, title=\"Energy data report\", dark_mode=True)\n",
        "# profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "QTxMV4tJsNEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize_variables(target, df_train, cat_numeric_unique_threshold=10):\n",
        "  target = target\n",
        "  categorical_numeric = [var for var in df_train.columns if df_train[var].dtype!='O' and var!=target and df_train[var].nunique()<=cat_numeric_unique_threshold]\n",
        "  continuous = [var for var in df_train.columns if df_train[var].dtype!='O' and var!=target and var not in categorical_numeric]\n",
        "  mixed = [var for var in df_train.columns if pd.api.types.infer_dtype(df_train[var]) == 'mixed']\n",
        "  categorical_object = [var for var in df_train.columns if df_train[var].dtype=='O' and var not in mixed]\n",
        "  sorted_features = [target]+categorical_numeric+continuous+categorical_object+mixed\n",
        "  print('Total columns: '+str(df_train.columns.size)+'\\nColumns after sorting: '+str(len(sorted_features)))\n",
        "  return target, categorical_numeric, continuous, mixed, categorical_object\n",
        "target, categorical_numeric, continuous, mixed, categorical_object = categorize_variables('total load actual', energy_df)"
      ],
      "metadata": {
        "id": "X8NyxaF2O1Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "energy_df[target] = energy_df[target].interpolate(method='linear')\n",
        "energy_df = energy_df[target].to_frame()"
      ],
      "metadata": {
        "id": "OihuQfc_i8gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights**\n",
        "* We left target variable as the only one, because we want to predict solely based on weather.\n",
        "* Target variable had several missing values, so we used linear interpolation to fill them."
      ],
      "metadata": {
        "id": "wohg6kFdzlRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target variable analysis"
      ],
      "metadata": {
        "id": "vi9067oCoUCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_decompose = seasonal_decompose(energy_df[:168][target], model='additive')\n",
        "plot = target_decompose.plot()\n",
        "plot.set_size_inches((16, 9))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0sNIQoCTWVQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_acf(energy_df[target], lags=5)\n",
        "plt.show()\n",
        "plot_pacf(energy_df[target], lags=5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s_laQ7-TqCIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights**\n",
        "* Series are non-stationary.\n",
        "* Target variable has daily seasonality.\n",
        "* The dataset has properties of AR process what stems from fast cutoff in PACF chart, this will become handy when we test ARIMA based models."
      ],
      "metadata": {
        "id": "JHVMaPrUPp59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weather dataset EDA"
      ],
      "metadata": {
        "id": "0Lx7GIkS9qa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df['dt_iso'] = pd.to_datetime(weather_df['dt_iso'], utc=True)"
      ],
      "metadata": {
        "id": "POGSbAnAFtKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# profile = ProfileReport(weather_df, title=\"Weather data report\")\n",
        "# profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "XVHn5xKQN460"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df.drop(['rain_3h','weather_main','weather_icon'], axis=1, inplace=True)\n",
        "weather_df = weather_df[weather_df['dt_iso'].isin(energy_df.index) & (~weather_df.duplicated(['dt_iso','city_name']))]"
      ],
      "metadata": {
        "id": "pmHKSqzXdBsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df = weather_df.set_index(['dt_iso','city_name'])\n",
        "weather_df = weather_df.unstack('city_name')\n",
        "weather_df.columns = ['_'.join(col).strip() for col in weather_df.columns.values]"
      ],
      "metadata": {
        "id": "iqnZmWzDzixd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Merging energy and weather dataset"
      ],
      "metadata": {
        "id": "HuxAgotD9723"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "energy_weather_df = energy_df.join(weather_df, how='inner')"
      ],
      "metadata": {
        "id": "6u7WPZ5X-rM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature selection and engineering"
      ],
      "metadata": {
        "id": "olSkAVtoVRbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforming variables"
      ],
      "metadata": {
        "id": "s0dnHMbZ-lUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target, categorical_numeric, continuous, mixed, categorical_object = categorize_variables('total load actual', energy_weather_df)\n",
        "\n",
        "train = energy_weather_df[:-168]\n",
        "test = energy_weather_df[-168:]\n",
        "\n",
        "encoder = OrdinalEncoder(\n",
        "    variables=categorical_object,\n",
        "    encoding_method='ordered'\n",
        ")\n",
        "\n",
        "train = encoder.fit_transform(train, train[target])\n",
        "test = encoder.fit_transform(test, test[target])"
      ],
      "metadata": {
        "id": "Eb98HwEgR7pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature selection"
      ],
      "metadata": {
        "id": "2Ea8uIrA-p85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "import eli5\n",
        "\n",
        "# params = {\n",
        "#     'objective': 'reg:squarederror',\n",
        "#     'random_state': 42,\n",
        "#     'n_jobs': -1,\n",
        "#     'learning_rate': 0.1,\n",
        "#     'max_depth': 3,\n",
        "#     'min_child_weight': 1,\n",
        "#     'gamma': 0,\n",
        "#     'subsample': 0.8,\n",
        "#     'colsample_bytree': 0.8,\n",
        "#     'reg_alpha': 0,\n",
        "#     'reg_lambda': 1\n",
        "# }\n",
        "\n",
        "xgb = XGBRegressor()\n",
        "xgb.fit(train[train.columns.difference([target])], train[target])\n",
        "\n",
        "display(eli5.show_weights(xgb, feature_names = test[test.columns.difference([target])].columns.tolist()))"
      ],
      "metadata": {
        "id": "v0x5DhLlhJ6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# lgb_regressor = lgb.LGBMRegressor()\n",
        "\n",
        "# feat_selector = BorutaPy(lgb_regressor, n_estimators='auto', random_state=42)\n",
        "# feat_selector.fit(X_train.values, y_train.values)\n",
        "# selected_rf_features = pd.DataFrame({'Feature':list(X_train.columns),\n",
        "#                                        'Ranking':feat_selector.ranking_}).sort_values(by='Ranking')\n",
        "# selected_rf_features.nsmallest(40, 'Ranking').plot.barh(x='Feature',figsize=(24,5))"
      ],
      "metadata": {
        "id": "8DsMlDoRXCsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model selection"
      ],
      "metadata": {
        "id": "YqHfxOvM5xnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we transform data into statsforecast format and also order columns for statsforecast framework so that it can use"
      ],
      "metadata": {
        "id": "wcGNH7lv6Bgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_statsforecast_format(df):\n",
        "  energy_weather_df_forecast = df.copy()\n",
        "  energy_weather_df_forecast['unique_id'] = 'Energy_weather_Spain'\n",
        "  energy_weather_df_forecast['ds'] = energy_weather_df_forecast.index.tz_localize(None)\n",
        "  energy_weather_df_forecast = energy_weather_df_forecast.rename(columns={'total load actual':'y'})\n",
        "\n",
        "  exogenous_variables = energy_weather_df_forecast.columns.difference(['unique_id', 'ds', 'y']).tolist()\n",
        "  cols = ['unique_id', 'ds', 'y'] + exogenous_variables\n",
        "  return energy_weather_df_forecast.reindex(columns=cols)\n",
        "\n",
        "train_forecast = transform_to_statsforecast_format(train)[-168:]\n",
        "test_forecast = transform_to_statsforecast_format(test)"
      ],
      "metadata": {
        "id": "P88MHBv46A4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ARIMA and others with statsforecast model test"
      ],
      "metadata": {
        "id": "bBTrSAQZPnRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how ARIMA with exogenous variables performs."
      ],
      "metadata": {
        "id": "xE1rwIfebamk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# season_length = 24\n",
        "\n",
        "# models = [\n",
        "#     AutoARIMA(season_length=season_length)\n",
        "# ]\n",
        "\n",
        "# sf = StatsForecast(\n",
        "#     models=models,\n",
        "#     freq='H',\n",
        "#     n_jobs=-1, verbose=True\n",
        "# )\n",
        "\n",
        "# crossvalidation_df = sf.cross_validation(\n",
        "#     df = train_forecast,\n",
        "#     h = len(test_forecast.index),\n",
        "#     n_windows = 3\n",
        "#   )\n",
        "\n",
        "# rmse = crossvalidation_df.groupby(['unique_id', 'cutoff']).apply(lambda df: rmse(df['y'], df['AutoARIMA'])).mean()\n",
        "\n",
        "# display(\"RMSE: \"+str(rmse))"
      ],
      "metadata": {
        "id": "I4tQyFHlt2XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cutoff = crossvalidation_df['cutoff'].unique()\n",
        "\n",
        "# for k in range(len(cutoff)):\n",
        "#     cv = crossvalidation_df[crossvalidation_df['cutoff'] == cutoff[k]]\n",
        "#     StatsForecast.plot(crossvalidation_df, cv.loc[:, cv.columns != 'cutoff'])"
      ],
      "metadata": {
        "id": "8xMowd9CeyHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see how ARIMA performs without exogenous variables just using target values for training."
      ],
      "metadata": {
        "id": "DxI6d-barMJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# crossvalidation_df = sf.cross_validation(\n",
        "#     df = train_forecast[['unique_id', 'ds', 'y']],\n",
        "#     h = len(test_forecast.index),\n",
        "#     n_windows = 3\n",
        "#   )"
      ],
      "metadata": {
        "id": "U0_t6IiEFOmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cutoff = crossvalidation_df['cutoff'].unique()\n",
        "\n",
        "# for k in range(len(cutoff)):\n",
        "#     cv = crossvalidation_df[crossvalidation_df['cutoff'] == cutoff[k]]\n",
        "#     StatsForecast.plot(crossvalidation_df, cv.loc[:, cv.columns != 'cutoff'])"
      ],
      "metadata": {
        "id": "KvKBTPmze4pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a result we can see that exogenous variables helped us to get a better RMSE result. So, it makes sense to use them."
      ],
      "metadata": {
        "id": "W5aI5i5Be5g6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tree model tests with mlforecast framework"
      ],
      "metadata": {
        "id": "9XOMVVsaOU5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tree model hyperparameters search"
      ],
      "metadata": {
        "id": "1G6oOR69MMLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# def objective(trial):\n",
        "\n",
        "#     model_name = trial.suggest_categorical(\"classifier\", ['LGBMRegressor', 'XGBRegressor'])\n",
        "\n",
        "#     if model_name == \"LGBMRegressor\":\n",
        "#       params = {\n",
        "#         \"objective\": \"regression\",\n",
        "#         \"metric\": \"rmse\",\n",
        "#         \"n_estimators\": 1000,\n",
        "#         \"verbosity\": -1,\n",
        "#         \"bagging_freq\": 1,\n",
        "#         \"learning_rate\": trial.suggest_float(\"learning_rate_light\", 1e-3, 0.1, log=True),\n",
        "#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 2**10),\n",
        "#         \"subsample\": trial.suggest_float(\"subsample_light\", 0.05, 1.0),\n",
        "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree_light\", 0.05, 1.0),\n",
        "#         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
        "#       }\n",
        "#       model = lgb.LGBMRegressor(**params)\n",
        "\n",
        "#     elif model_name == \"XGBRegressor\":\n",
        "#       params = {\n",
        "#       'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
        "#       'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
        "#       'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
        "#       'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "#       'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
        "#       'subsample': trial.suggest_loguniform('subsample', 0.01, 1.0),\n",
        "#       'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.01, 1.0),\n",
        "#       'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n",
        "#       'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n",
        "#       'eval_metric': 'rmse',\n",
        "#       'use_label_encoder': False\n",
        "#       }\n",
        "#       model = xgb.XGBRegressor(**params)\n",
        "\n",
        "#     ml_forecast = MLForecast(models=[model],\n",
        "#                    freq='H',\n",
        "#                    lags=[1,2,12,24,168],\n",
        "#                    date_features=['hour','day','week', 'month'],\n",
        "#                    num_threads=6)\n",
        "\n",
        "#     crossvalidation_df = ml_forecast.cross_validation(\n",
        "#                                                       data=train,\n",
        "#                                                       window_size=168,\n",
        "#                                                       n_windows=3,\n",
        "#                                                     )\n",
        "#     rmse = crossvalidation_df.groupby(['unique_id', 'cutoff']).apply(lambda df: rmse(df['y'], df[model_name])).mean()\n",
        "\n",
        "#     return rmse\n",
        "\n",
        "# study = optuna.create_study(direction=\"minimize\")\n",
        "# study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "# display(study.best_params)\n",
        "# display(study.best_value)"
      ],
      "metadata": {
        "id": "CkGOl4aEk130"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing tree models"
      ],
      "metadata": {
        "id": "egkR7k_-MfEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBRegressor()\n",
        "lgbm = lgb.LGBMRegressor()\n",
        "\n",
        "ml_forecast = MLForecast(models=[xgb,lgbm],\n",
        "                   freq='H',\n",
        "                   lags=[1,2,3,12,24],\n",
        "                   date_features=['hour','day','week'],\n",
        "                   num_threads=6)\n",
        "\n",
        "crossvalidation_df = ml_forecast.cross_validation(\n",
        "    df=train_forecast,\n",
        "    h=24,\n",
        "    n_windows=3\n",
        ")\n",
        "\n",
        "for model_name in ml_forecast.models:\n",
        "  rmse_value = crossvalidation_df.groupby(['unique_id', 'cutoff']).apply(lambda df: rmse(df['y'], df[model_name])).mean()\n",
        "\n",
        "  display(model_name+\" RMSE: \"+str(rmse_value))\n",
        "\n",
        "# ml_forecast.fit(train_forecast, static_features=['unique_id'], target_col='y')\n",
        "# y_pred = ml_forecast.predict(h=len(test_forecast.index), X_df=test_forecast[exogenous_variables])\n",
        "\n",
        "# for model_name in ml_forecast.models:\n",
        "#   display(model_name+\" RMSE: \"+str(rmse(test_forecast['y'].reset_index(drop=True), y_pred[model_name])))\n",
        "\n",
        "#   pd.Series(ml_forecast.models_[model_name].feature_importances_, index=ml_forecast.ts.features_order_).sort_values(ascending=False).plot.bar(figsize=(32, 6),title=f'{model_name} feature importance')\n",
        "#   plt.show()\n",
        "\n",
        "#   display(eli5.show_weights(ml_forecast.models_[model_name], feature_names = ml_forecast.ts.features_order_))\n",
        "\n",
        "#   plt.figure(figsize=(30, 6))\n",
        "#   sns.lineplot(x=test_forecast.index, y=test_forecast['y'], label='Real')\n",
        "#   sns.lineplot(x=test_forecast.index, y=y_pred[model_name], label='Predicted')\n",
        "#   plt.title('Real vs Predicted Values')\n",
        "#   plt.legend()\n",
        "#   plt.show()"
      ],
      "metadata": {
        "id": "8CEoZH4GRVjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DL model hyperparameter search"
      ],
      "metadata": {
        "id": "sT3dDeLWYY7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# def objective(trial):\n",
        "\n",
        "#     model_name = trial.suggest_categorical(\"classifier\", ['LGBMRegressor', 'XGBRegressor'])\n",
        "\n",
        "#     if model_name == \"LGBMRegressor\":\n",
        "#       params = {\n",
        "#         \"objective\": \"regression\",\n",
        "#         \"metric\": \"rmse\",\n",
        "#         \"n_estimators\": 1000,\n",
        "#         \"verbosity\": -1,\n",
        "#         \"bagging_freq\": 1,\n",
        "#         \"learning_rate\": trial.suggest_float(\"learning_rate_light\", 1e-3, 0.1, log=True),\n",
        "#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 2**10),\n",
        "#         \"subsample\": trial.suggest_float(\"subsample_light\", 0.05, 1.0),\n",
        "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree_light\", 0.05, 1.0),\n",
        "#         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
        "#       }\n",
        "#       model = lgb.LGBMRegressor(**params)\n",
        "\n",
        "#     elif model_name == \"XGBRegressor\":\n",
        "#       params = {\n",
        "#       'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
        "#       'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
        "#       'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
        "#       'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "#       'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
        "#       'subsample': trial.suggest_loguniform('subsample', 0.01, 1.0),\n",
        "#       'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.01, 1.0),\n",
        "#       'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n",
        "#       'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n",
        "#       'eval_metric': 'rmse',\n",
        "#       'use_label_encoder': False\n",
        "#       }\n",
        "#       model = xgb.XGBRegressor(**params)\n",
        "\n",
        "#     ml_forecast = MLForecast(models=[model],\n",
        "#                    freq='H',\n",
        "#                    lags=[1,2,12,24,168],\n",
        "#                    date_features=['hour','day','week', 'month'],\n",
        "#                    num_threads=6)\n",
        "\n",
        "#     crossvalidation_df = ml_forecast.cross_validation(\n",
        "#                                                       data=train,\n",
        "#                                                       window_size=168,\n",
        "#                                                       n_windows=3,\n",
        "#                                                     )\n",
        "#     rmse = crossvalidation_df.groupby(['unique_id', 'cutoff']).apply(lambda df: rmse(df['y'], df[model_name])).mean()\n",
        "\n",
        "#     return rmse\n",
        "\n",
        "# study = optuna.create_study(direction=\"minimize\")\n",
        "# study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "# display(study.best_params)\n",
        "# display(study.best_value)"
      ],
      "metadata": {
        "id": "SZO1Tk8nYUSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DL model testing"
      ],
      "metadata": {
        "id": "xrrmjltkVGDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch, gc\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "bXCyMnOdkH1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [LSTM(h=len(test_forecast.index),\n",
        "               loss=DistributionLoss(distribution='Normal', level=[90]),\n",
        "               max_steps=50,\n",
        "               encoder_n_layers=2,\n",
        "               encoder_hidden_size=200,\n",
        "               context_size=8760,\n",
        "               decoder_hidden_size=200,\n",
        "               decoder_layers=2,\n",
        "               learning_rate=1e-3,\n",
        "               scaler_type='standard',\n",
        "               hist_exog_list=train_forecast[train_forecast.columns.difference([target])],\n",
        "               futr_exog_list=train_forecast[train_forecast.columns.difference([target])])]\n",
        "\n",
        "neural_forecast = NeuralForecast(models=models, freq='H')\n",
        "\n",
        "crossvalidation_df = neural_forecast.cross_validation(train_forecast, n_windows=3)\n",
        "\n",
        "rmse = crossvalidation_df.groupby(['unique_id', 'cutoff']).apply(lambda df: rmse(df['y'], df['LSTM'])).mean()\n",
        "\n",
        "display(\"RMSE: \"+str(rmse))\n",
        "\n",
        "#neural_forecast.fit(train_forecast)\n",
        "# y_pred = neural_forecast.predict(futr_df=test_forecast[test_forecast.columns.difference(['y'])])\n",
        "\n",
        "# for model_name in neural_forecast.models:\n",
        "#   display(model_name+\" RMSE: \"+str(rmse(test_forecast['y'].reset_index(drop=True), y_pred[model_name])))\n",
        "\n",
        "#   pd.Series(neural_forecast.models_[model_name].feature_importances_, index=neural_forecast.ts.features_order_).sort_values(ascending=False).plot.bar(figsize=(32, 6),title=f'{model_name} feature importance')\n",
        "#   plt.show()\n",
        "\n",
        "#   display(eli5.show_weights(neural_forecast.models_[model_name], feature_names = neural_forecast.ts.features_order_))\n",
        "\n",
        "#   plt.figure(figsize=(30, 6))\n",
        "#   sns.lineplot(x=test_forecast.index, y=test_forecast['y'], label='Real')\n",
        "#   sns.lineplot(x=test_forecast.index, y=y_pred[model_name], label='Predicted')\n",
        "#   plt.title('Real vs Predicted Values')\n",
        "#   plt.legend()\n",
        "#   plt.show()"
      ],
      "metadata": {
        "id": "5tDj99QeVCJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred =  neural_forecast.predict(futr_df=test)\n",
        "\n",
        "# for model_name in neural_forecast.models:\n",
        "#   display(mean_squared_error(test['y'], y_pred[model_name], squared=False))\n",
        "\n",
        "#   pd.Series(neural_forecast.models_[model_name].feature_importances_, index=neural_forecast.ts.features_order_).sort_values(ascending=False).plot.bar(figsize=(32, 6),title=f'{model_name} feature importance')\n",
        "#   plt.show()\n",
        "\n",
        "#   plt.figure(figsize=(30, 6))\n",
        "#   sns.lineplot(x=test.index, y=test['y'], label='Real')\n",
        "#   sns.lineplot(x=test.index, y=y_pred[model_name], label='Predicted')\n",
        "#   plt.title('Real vs Predicted Values')\n",
        "#   plt.legend()\n",
        "#   plt.show()"
      ],
      "metadata": {
        "id": "uz3L9tXjwekF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}