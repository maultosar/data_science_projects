{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport seaborn as sns\nimport missingno as msno\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom sklearn import preprocessing\n!pip3 install dataprep\nfrom dataprep.eda import plot, plot_correlation, create_report, plot_missing\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ndf_test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\ndf_train.drop(['Id'], axis=1, inplace=True)\ndf_test.drop(['Id'], axis=1, inplace=True)\ndf_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_report(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Duplicate removal \ndf_train.drop_duplicates(inplace=True)\n\n# Filling missing values\nstring_fill = ['PoolQC','Alley','Fence','MiscFeature','FireplaceQu','GarageYrBlt','GarageCond','GarageType','GarageFinish',\n               'GarageQual','BsmtExposure','BsmtFinType2','BsmtCond','BsmtQual','BsmtFinType1','MasVnrType','Electrical']\nnumeric_fill = ['LotFrontage','MasVnrArea']\n\ndf_train[string_fill] = df_train[string_fill].fillna(value='Missing')\ndf_train[numeric_fill] = df_train[numeric_fill].fillna(value=-1)\ndf_test[string_fill] = df_test[string_fill].fillna(value='Missing')\ndf_test[numeric_fill] = df_test[numeric_fill].fillna(value=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = 'SalePrice'\ncategorical_numeric = [var for var in df_train.columns if df_train[var].dtype!='O' and var!=target and df_train[var].nunique()<10]\ncontinuous = [var for var in df_train.columns if df_train[var].dtype!='O' and var!=target and var not in categorical_numeric]\ncategorical_object = [var for var in df_train.columns if df_train[var].dtype=='O']\nsorted_features = [target]+categorical_numeric+continuous+categorical_object\nprint('Total columns: '+str(df_train.columns.size)+'\\nColumns after sorting: '+str(len(sorted_features)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"### Categorical features analysis","metadata":{}},{"cell_type":"code","source":"# Categorical value counts \nplot(df_train[categorical_object+categorical_numeric])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Continuous features analysis","metadata":{}},{"cell_type":"code","source":"# Continuous variables analysis\nplot(df_train[continuous])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target variable analysis","metadata":{}},{"cell_type":"code","source":"plot(df_train,target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature selection","metadata":{}},{"cell_type":"code","source":"!pip3 install feature_engine","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from feature_engine.selection import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = df_train.copy()[df_train.columns.difference([target])]\ny_train = df_train[target]\nX_test = df_test.copy()[df_test.columns.difference([target])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop duplicate features","metadata":{}},{"cell_type":"code","source":"sel = DropDuplicateFeatures(variables=None, missing_values='raise') \nsel.fit(X_train)\nX_train = sel.transform(X_train) \nX_test = sel.transform(X_test)\nsel.features_to_drop_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Constant/quasi-constant features","metadata":{}},{"cell_type":"code","source":"sel = DropConstantFeatures(tol=0.998, variables=None, missing_values='raise')\nsel.fit(X_train)\nX_train = sel.transform(X_train)\nX_test = sel.transform(X_test)\nsel.features_to_drop_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature correlation","metadata":{}},{"cell_type":"code","source":"# Model performance based correlated features removal\n# random forest \nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor( \n    n_estimators=10, \n    random_state=20, \n    n_jobs=4, \n) \n# correlation selector \nsel = SmartCorrelatedSelection( \n    variables=None, # if none, selector examines all numerical variables \n    method=\"pearson\", \n    threshold=0.8, \n    missing_values=\"raise\", \n    selection_method=\"model_performance\", \n    estimator=rf, \n    scoring=\"r2\", \n    cv=3, \n) \n# this may take a while, because we are training \n# a random forest per correlation group \nsel.fit(X_train, y_train)\nX_train = sel.transform(X_train)  \nX_test = sel.transform(X_test)\nsel.correlated_feature_sets_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Continuous target","metadata":{}},{"cell_type":"code","source":"# Dataprep version \nfor column in categorical_object+categorical_numeric: \n    plot(df_train,column,target).show() \nfor column in continuous: \n    plot(df_train,column,target).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation verification","metadata":{}},{"cell_type":"code","source":"# Dataprep version\nplot_correlation(df_train[continuous+[target]])\n\n# Correlation between categorical features and continuous target - ANOVA\n#TODO for feature vs feature \nfrom sklearn.feature_selection import SelectKBest, f_regression \nselection_model = SelectKBest(f_regression, k=40).fit_transform(X_train, y_train)\nmask = selection_model.get_support()\ndisplay('Most important features: ': X_train.columns[mask])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Boruta feature selection","metadata":{}},{"cell_type":"code","source":"!pip3 install Boruta","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom boruta import BorutaPy\nfrom sklearn.preprocessing import OrdinalEncoder\n\nX_train_boruta = X_train.copy()\n\nenc = OrdinalEncoder()\nX_train_boruta[categorical_object] = enc.fit_transform(X_train_boruta[categorical_object])\n\nrf_b = RandomForestRegressor(n_jobs=-1, class_weight='balanced', max_depth=5)\nfeat_selector = BorutaPy(rf_b, n_estimators='auto', verbose=2, random_state=1)\nfeat_selector.fit(X_train_boruta.values, y_train.values)\nfeat_selector.support_\ndisplay(feat_selector.ranking_)\n# X_filtered = feat_selector.transform(X_train)\n\nprint(\"\\n------Support and Ranking for each feature------\") \nfor i in range(len(feat_selector.support_)): \n    if feat_selector.support_[i]: \n        print(\"Passes the test: \", X_train.columns[i], \n              \" - Ranking: \", feat_selector.ranking_[i]) \n    else: \n        print(\"Doesn't pass the test: \", \n              X_train.columns[i], \" - Ranking: \", feat_selector.ranking_[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model optimization and selection with optuna","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\ndef objective(trial): \n    \n    classifier_name = trial.suggest_categorical(\"classifier\", [\"random_forest\", 'XGBoost'])\n     \n#     if classifier_name == \"logit\": \n         \n#         logit_penalty = trial.suggest_categorical('logit_penalty', ['l1','l2']) \n#         logit_c = trial.suggest_float('logit_c', 0.001, 10) \n#         logit_solver = 'saga' \n         \n#         model = LogisticRegression( \n#             penalty=logit_penalty, \n#             C=logit_c, \n#             solver=logit_solver, \n#         ) \n         \n#     elif classifier_name ==\"RF\": \n         \n#         rf_n_estimators = trial.suggest_int(\"rf_n_estimators\", 100, 1000) \n#         rf_criterion = trial.suggest_categorical(\"rf_criterion\", ['gini', 'entropy']) \n#         rf_max_depth = trial.suggest_int(\"rf_max_depth\", 1, 4) \n#         rf_min_samples_split = trial.suggest_float(\"rf_min_samples_split\", 0.01, 1) \n#         model = RandomForestClassifier( \n#             n_estimators=rf_n_estimators, \n#             criterion=rf_criterion, \n#             max_depth=rf_max_depth, \n#             min_samples_split=rf_min_samples_split, \n#         ) \n         \n#     if classifier_name == 'GBM': \n         \n#         gbm_n_estimators = trial.suggest_int(\"gbm_n_estimators\", 100, 1000) \n#         gbm_criterion = trial.suggest_categorical(\"gbm_criterion\", ['mse', 'friedman_mse']) \n#         gbm_max_depth = trial.suggest_int(\"gbm_max_depth\", 1, 4) \n#         gbm_min_samples_split = trial.suggest_float(\"gbm_min_samples_split\", 0.01, 1) \n#         model = GradientBoostingClassifier( \n#             n_estimators=gbm_n_estimators, \n#             criterion=gbm_criterion, \n#             max_depth=gbm_max_depth, \n#             min_samples_split=gbm_min_samples_split, \n#         ) \n        \n#     else:\n        \n        if classifier_name == \"random_forest\":\n            params = {\n                'n_estimators' : trial.suggest_int('n_estimators', 100, 500),\n                'min_samples_split' : trial.suggest_int('min_samples_split', 3, 10),\n                'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 3, 10),\n                'max_features' : trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\"]),\n                'max_depth' : trial.suggest_int('max_depth', 3, 10),\n                'bootstrap' : True,\n                'random_state' : 42\n            }\n            model = RandomForestRegressor(**params)\n            \n        else:\n            params = {\n                'max_depth': trial.suggest_int('max_depth', 1, 10),\n                'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n                'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n                'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n                'subsample': trial.suggest_loguniform('subsample', 0.01, 1.0),\n                'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.01, 1.0),\n                'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n                'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n                'eval_metric': 'r2_score', #TODO what if I want to minimize some metrics?\n                'use_label_encoder': False\n            }\n\n            # Fit the model\n            model = XGBRegressor(**params)\n     \n    score = cross_val_score(model, X_train, y_train, cv=3) \n    average_score = score.mean() \n     \n    return average_score\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\n\ndisplay(study.best_params)\ndisplay(study.best_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from xgboost import XGBRegressor\n\n# params = [\n\n# ]\n\n# for i, param in enumerate(params):\n#     model = XGBRegressor(**param)\n\n#     model.fit(X_train, y_train)\n#     y_pred = model.predict(X_test)\n\n#     output = pd.DataFrame({\n#     \"Id\": pd.read_csv('\"../input/house-prices-advanced-regression-techniques/test.csv\"')['Id'],\n#     \"SalePrice\": y_pred\n#     })\n\n#     output.to_csv(f'{i}submission.csv', index=False)\n#     display(\"Your submission was successfully saved!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}